 2/1: i%paste -q
 2/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
 2/3:
# windows machine at the office
project_path = "D:/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nick/9D47-AA7C/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nicholasw/3AC0-16FE/Summer 2022 (C4GC with BII)/measles_metapop/{}"
 2/4:
import sys
sys.path.append(project_path.format("scripts/"))
 2/5: from spatial_tsir import *
 2/6: i%paste -q
 2/7: i%paste -q
 2/8: i%paste -q
 2/9: %paste -q
 3/1: %paste -q
 4/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# windows machine at the office
project_path = "D:/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nick/9D47-AA7C/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nicholasw/3AC0-16FE/Summer 2022 (C4GC with BII)/measles_metapop/{}"

import sys
sys.path.append(project_path.format("scripts/"))

from spatial_tsir import *
 4/3: %paste -q
 5/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# windows machine at the office
project_path = "D:/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nick/9D47-AA7C/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nicholasw/3AC0-16FE/Summer 2022 (C4GC with BII)/measles_metapop/{}"

import sys
sys.path.append(project_path.format("scripts/"))

from spatial_tsir import *
 5/5: %paste -q
 6/5:
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)

# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)

# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
 6/7: vacc_df
 6/8: %paste -q
 7/8:
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)

# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)

# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
7/10: %paste -q
8/10:
# reasonable to start with constant beta/contact rate

# retrieved from "A Gravity Model for Epidemic Metapopulations"
beta_t = [1.24, 1.14,1.16,1.31,1.24,1.12,1.06,1.02,0.94,0.98,1.06,1.08,
          0.96,0.92,0.92,0.86,0.76,0.63,0.62,0.83,1.13,1.20,1.11,1.02,1.04,1.08]
beta_t = np.array(beta_t)*30

# birth rate?
# https://www.marchofdimes.org/peristats/data?reg=99&top=2&stop=1&lev=1&slev=4&obj=1&sreg=51
birth_rate = 60/1000/28

params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))

config = {
    "iters":50,
    
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":7
    #"birth":birth_rate
    #"beta_t":beta_t
    #"birth_t":cases['birth_per_cap']
}
8/12: %paste -q
9/12:
initial_state = np.zeros((len(vacc_df.index),2))

R = vacc_df['pop'] - vacc_df['nVaccCount']

# one way - again, try seeding top 5 counties
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
# another way - try seeding the top 5 unvaccinated counties (by raw numbers)
#top_5 = vacc_df.sort_values(by='nVaccCount',ascending=False).head(5)
# another way - try seeding the top 5 unvaccinated counties (by ratio)
#vacc_df['ratio'] = vacc_df['nVaccCount']/vacc_df['pop']
#top_5 = vacc_df.sort_values(by='ratio',ascending=False).head(5)

I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)

initial_state[:,0] = I
initial_state[:,1] = R
9/14: %paste -q
10/14:
sim = spatial_tSIR(config,
        vacc_df,
        initial_state,
        distances=np.array(dist_mat))

sim.run_simulation()
10/16: sim.plot_epicurve()
10/17: %matplotlib qt
11/1: %paste -q
12/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# windows machine at the office
project_path = "D:/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nick/9D47-AA7C/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nicholasw/3AC0-16FE/Summer 2022 (C4GC with BII)/measles_metapop/{}"

import sys
sys.path.append(project_path.format("scripts/"))

from spatial_tsir import *
12/3: %paste -q
13/3:
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)

# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)

# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
13/5: %paste -q
14/5:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# windows machine at the office
project_path = "D:/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nick/9D47-AA7C/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/run/media/nicholasw/3AC0-16FE/Summer 2022 (C4GC with BII)/measles_metapop/{}"

import sys
sys.path.append(project_path.format("scripts/"))

from spatial_tsir import *
14/7: %paste -q
15/7:
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)

# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)

# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
15/9: %paste -q
16/9:
# reasonable to start with constant beta/contact rate

# retrieved from "A Gravity Model for Epidemic Metapopulations"
beta_t = [1.24, 1.14,1.16,1.31,1.24,1.12,1.06,1.02,0.94,0.98,1.06,1.08,
          0.96,0.92,0.92,0.86,0.76,0.63,0.62,0.83,1.13,1.20,1.11,1.02,1.04,1.08]
beta_t = np.array(beta_t)*30

# birth rate?
# https://www.marchofdimes.org/peristats/data?reg=99&top=2&stop=1&lev=1&slev=4&obj=1&sreg=51
birth_rate = 60/1000/28

params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))

config = {
    "iters":50,
    
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":7
    #"birth":birth_rate
    #"beta_t":beta_t
    #"birth_t":cases['birth_per_cap']
}
16/11: %paste -q
17/11:
initial_state = np.zeros((len(vacc_df.index),2))

R = vacc_df['pop'] - vacc_df['nVaccCount']

# one way - again, try seeding top 5 counties
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
# another way - try seeding the top 5 unvaccinated counties (by raw numbers)
#top_5 = vacc_df.sort_values(by='nVaccCount',ascending=False).head(5)
# another way - try seeding the top 5 unvaccinated counties (by ratio)
#vacc_df['ratio'] = vacc_df['nVaccCount']/vacc_df['pop']
#top_5 = vacc_df.sort_values(by='ratio',ascending=False).head(5)

I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)

initial_state[:,0] = I
initial_state[:,1] = R
17/13: %paste -q
18/13:
sim = spatial_tSIR(config,
        vacc_df,
        initial_state,
        distances=np.array(dist_mat))

sim.run_simulation()
18/15: %matplotlib qt
18/16: sim.plot_epicurve()
18/17: %cpaste -q
19/17:
#%% multithread simulation
# test
sim_pool = spatial_tSIR_pool(config,
        vacc_df,
        initial_state,
        n_sim=100,
        distances=np.array(dist_mat))
sim_pool.run_simulation(threads=4)
19/19: %cpaste -q
20/19:
#%% multithread simulation
# test
sim_pool = spatial_tSIR_pool(config,
        vacc_df,
        initial_state,
        n_sim=100,
        distances=np.array(dist_mat))
with multiprocess.Pool(5) as p:
    sim_pool.run_simulation(pool=p)
20/21: import multiprocess
20/22: %cpaste -q
21/22:
with multiprocess.Pool(5) as p:
    sim_pool.run_simulation(pool=p)
21/24: sim_pool.get_samples()
21/25: plt.plot(sim_pool.get_samples())
21/26: plt.plot(sim_pool.get_samples().T)
21/27: sim_pool.plot_interval()
21/28: plt.figure()
21/29: sim_pool.plot_survival()
21/30: sim_pool.plot_survival()
21/31: sim_pool.plot_interval()
22/1: {'f':3,'g':4}
22/2: {'f':3,'g':4}.keys()
22/3: intersect ['f'] {'f':3,'g':4}.keys()
22/4: np.abs(3)
22/5: import numpy as np
22/6: np.abs(3)
23/1: import numpy as np
23/2: np.array([1,2,3])
23/3: np.stack([1,2,3],[4,5,6])
23/4: np.stack(np.array([1,2,3]),np.array([4,5,6]))
23/5: np.stack
23/6: help
23/7: help(np.stack)
23/8: np.stack([np.array([1,2,3]),np.array([4,5,6])])
23/9: type(None)
23/10: np.stack([np.array([[1,2,3],[4,5,6]]),np.array([4,5,6])])
23/11: np.stack([np.array([[1,2,3],[4,5,6]]),np.array([[4,5,6]])])
23/12: np.stack([np.array([[1,2,3],[4,5,6]]),np.array([[4,5,6]])],axis=1)
23/13: np.stack([np.array([[1,2,3],[4,5,6]]),np.array([[4,5,6]])],axis=0)
23/14: np.concat([np.array([[1,2,3],[4,5,6]]),np.array([[4,5,6]])],axis=0)
23/15: np.concatenate([np.array([[1,2,3],[4,5,6]]),np.array([[4,5,6]])],axis=0)
23/16: np.savetxt
23/17: np.savetxt(np.array([1,2,3]))
23/18: np.savetxt(np.array([1,2,3]),delimiter=",")
23/19: np.array_str(np.array([1,2,3]))
23/20: np.array2string(np.array([1,2,3]))
23/21: np.fromstring(np.array2string(np.array([1,2,3])))
23/22: np.frombuffer(np.array2string(np.array([1,2,3])))
23/23: np.array([1,2,3])
23/24: np.array([1,2,3])
23/25: import pandas as pd
23/26: np.array([1,2,3])
23/27: "".join(np.array([1,2,3]))
23/28: "".join(str(np.array([1,2,3])))
23/29: np.array([1,2,3])
23/30: np.array2string(np.array([1,2,3]))
23/31: np.array2string(np.array([1,2,3]),seperator=";")
23/32: np.array2string(np.array([1,2,3]),separator=";")
23/33: np.array2string(np.array([1,2,3]),separator="_")
23/34: np.ones(5)
23/35: np.ones((5,5))
23/36: np.concatenate(np.ones((5,5)),np.zeros((5,5)))
23/37: np.concatenate(np.ones((5,5)),np.zeros((5,5)),axis=0)
23/38: np.concatenate([np.ones((5,5)),np.zeros((5,5))],axis=0)
23/39: np.concatenate([np.ones((5,5)),np.zeros((5,5))],axis=1)
23/40: !pip install pandas
23/41: %pip install pandas
23/42: import pandas as pd
23/43: pd.DataFrame(np.concatenate([np.ones((5,5)),np.zeros((5,5))],axis=0))
23/44: pd.DataFrame(np.concatenate([np.ones((5,5)),np.zeros((5,5))],axis=1))
23/45: test = pd.DataFrame(np.concatenate([np.ones((5,5)),np.zeros((5,5))],axis=1))
23/46: test.columns[0]='asdf'
23/47: test.columns[0]='asdf'
23/48: test.rename(columns={0:'hi'})
23/49: test
23/50: np.zeros((5,6))
25/1: %cpaste -q
26/1:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
26/3: %cpaste -q
27/3:
import numpy as np
import warnings
import dill
import pandas as pd
# TODO Need to import spatial tSIR module. is there a better way to do this??
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/"))
from spatial_tsir import *
"""
Write a wrapper for the stochastic tSIR that takes in different representations
of the vaccination vector and runs the simulation.
"""
class VaccRateOptEngine:
    """
    Wrapper class that should make it easier to 
    query the spatial tSIR model in an optimization problem.
    It serves as the "oracle" that you can query during the optimization routine.
    """
    # def __init__(self,
    #         V_0,inf_seed,
    #         sim_config, pop, distances,
    #         obj,V_repr,
    #         aux_param):
    def __init__(self,
            opt_config, V_0, seed,
            sim_config, pop, distances):
        # TODO: fill out the rest of the docstring
        """
        Create a new optimization oracle, initializing with certain fixed parameters.
27/5: %cpaste -q
28/5:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
28/7: %cpaste -q
29/7:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
29/9: %cpaste -q
30/9:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
30/11: %cpaste -q
32/11:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
33/1: %cpaste -q
34/1:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
34/3:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
35/1: %cpaste -q
36/1:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
37/1: %cpaste -q
38/1:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
39/1: %cpaste -q
40/1:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
40/3: %cpaste -q
41/3:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
41/5: %cpaste -q
42/5:
#tSIR parameters
params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))
tsir_config = {
    "iters":75,
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":3
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
42/7: %cpaste -q
43/7:
#%%
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
# optimization parameters
opt_config = {
    'obj':"attacksize",
    'V_repr':"max_ratio",
    'constraint_bnd':0.05,
    "attacksize_cutoff":1000
}
43/9: %cpaste -q
44/9:
#%%
import multiprocess
44/11: %cpaste -q
45/11:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
45/13: %cpaste -q
46/13:
with multiprocess.Pool(12) as p:
    engine.query(engine.V_0,pool=p,n_sim=50)
    result = engine.query(V_prime,pool=p,n_sim=50)
46/15: result
46/16: engine
46/17: engine.eval_history
46/18: engine.eval_history['output']
46/19: engine.eval_history['output'][0]
46/20: np.mean(engine.eval_history['output'][0])
46/21: np.mean(engine.eval_history['output'][1])
46/22: %cpaste -q
47/22:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
47/24: %cpaste -q
48/24:
with multiprocess.Pool(7) as p:
    [engine.query(engine.V_0,pool=p,n_sim=50) for i in range(0,50)]
    #result = engine.query(V_prime,pool=p,n_sim=50)
48/26: engine
48/27: engine.eval_history
48/28: engine.eval_history['output']
48/29: engine.eval_history['output'][0]
48/30: engine.eval_history['output'].shape
48/31: np.mean(engine.eval_history['output'])
48/32: np.mean(engine.eval_history['output'],axis=1)
48/33: np.mean(engine.eval_history['output'],axis=0)
48/34: plt.hist(np.mean(engine.eval_history['output'],axis=0))
48/35: import matplotlib.pyplot as plt
48/36: plt.hist(np.mean(engine.eval_history['output'],axis=0))
48/37: %matplotlib qt
48/38: plt.hist(np.mean(engine.eval_history['output'],axis=0))
48/39: import seaborn as sns
48/40: sns.kdeplot(np.mean(engine.eval_history['output'],axis=0))
48/41: sns.kdeplot(np.mean(engine.eval_history['output'],axis=0))
48/42: plt.hist(np.mean(engine.eval_history['output'],axis=0),density=True)
48/43: engine.eval_history['output']
48/44: np.std(engine.eval_history['output'])
50/1: %cpaste -q
51/1:
def move_seed(S,budget=None):
    if budget is None:
        budget = np.sum(S)
    nonzero_indices = np.nonzero(S)
    nonzero_vals = S[nonzero_indices]
    rand_select = randint.rvs(0,budget)
    # what index does that correspond to?
    # first index where rolling sum > rand_select
    rolling_sum = np.cumsum(nonzero_vals)
    index_to_subtract = nonzero_indices[np.argmax(rolling_sum>rand_select)]
    index_to_add = randint.rvs(0,len(S)-1)
    to_return = S
    to_return[index_to_subtract] = to_return[index_to_subtract]-1
    to_return[index_to_add] = to_return[index_to_add]+1
    return to_return
51/3: import numpy as np
51/4: np.array([1,1,1,0,0,1])
51/5: move_seed(np.array([1,1,1,0,0,1]))
51/6: %pip install scipy
51/7: %cpaste -q
52/7:
import numpy as np
from scipy.stats import uniform, randint
52/9: move_seed(np.array([1,1,1,0,0,1]))
52/10: np.array([False,False,True,False])
52/11: np.array([False,False,True,True])
52/12: np.argmax(np.array([False,False,True,True]))
52/13: move_seed(np.array([1,1,1,0,0,1]))
52/14: move_seed(np.array([1,1,2,0,0,1]))
52/15: %cpaste -q
53/15:
def move_seed(S,budget=None):
    if budget is None:
        budget = np.sum(S)
    nonzero_indices = np.nonzero(S)
    nonzero_vals = S[nonzero_indices]
    rand_select = randint.rvs(0,budget)
    # what index does that correspond to?
    # first index where rolling sum > rand_select
    rolling_sum = np.cumsum(nonzero_vals)
    print(np.argmax(rolling_sum>rand_select))
    index_to_subtract = nonzero_indices[np.argmax(rolling_sum>rand_select)]
    index_to_add = randint.rvs(0,len(S)-1)
    to_return = S
    to_return[index_to_subtract] = to_return[index_to_subtract]-1
    to_return[index_to_add] = to_return[index_to_add]+1
    return to_return
53/17: move_seed(np.array([1,1,2,0,0,1]))
53/18: move_seed(np.array([1,1,2,0,0,1]))
53/19: %cpaste -q
54/19:
def move_seed(S,budget=None):
    if budget is None:
        budget = np.sum(S)
    nonzero_indices = np.nonzero(S)
    nonzero_vals = S[nonzero_indices]
    rand_select = randint.rvs(0,budget)
    # what index does that correspond to?
    # first index where rolling sum > rand_select
    rolling_sum = np.cumsum(nonzero_vals)
    print(np.argmax(rolling_sum>rand_select))
    print(nonzero_indices)
    index_to_subtract = nonzero_indices[np.argmax(rolling_sum>rand_select)]
    index_to_add = randint.rvs(0,len(S)-1)
    to_return = S
    to_return[index_to_subtract] = to_return[index_to_subtract]-1
    to_return[index_to_add] = to_return[index_to_add]+1
    return to_return
54/21: move_seed(np.array([1,1,2,0,0,1]))
54/22: move_seed(np.array([1,1,2,0,0,1]))
54/23: %cpaste -q
55/23:
def move_seed(S,budget=None):
    if budget is None:
        budget = np.sum(S)
    nonzero_indices = np.nonzero(S)[0]
    nonzero_vals = S[nonzero_indices]
    rand_select = randint.rvs(0,budget)
    # what index does that correspond to?
    # first index where rolling sum > rand_select
    rolling_sum = np.cumsum(nonzero_vals)
    print(np.argmax(rolling_sum>rand_select))
    print(nonzero_indices)
    index_to_subtract = nonzero_indices[np.argmax(rolling_sum>rand_select)]
    index_to_add = randint.rvs(0,len(S)-1)
    to_return = S
    to_return[index_to_subtract] = to_return[index_to_subtract]-1
    to_return[index_to_add] = to_return[index_to_add]+1
    return to_return
55/25: move_seed(np.array([1,1,2,0,0,1]))
55/26: move_seed(np.array([1,1,2,0,0,1]))
55/27: move_seed(np.array([1,1,2,0,0,1]))
55/28: move_seed(np.array([1,1,2,0,0,1]))
55/29: move_seed(np.array([1,1,2,0,0,1]))
55/30: move_seed(np.array([1,1,2,0,0,1]))
55/31: move_seed(np.array([1,1,2,0,0,1]))
55/32: move_seed(np.array([1,1,2,0,0,1]))
55/33: move_seed(np.array([1,1,2,0,0,1]))
55/34: move_seed(np.array([1,1,2,0,0,1]))
55/35: move_seed(np.array([1,1,2,0,0,1]))
55/36: move_seed(np.array([1,1,2,0,0,1]))
55/37: move_seed(np.array([1,1,2,0,0,1]))
55/38: move_seed(np.array([1,1,2,0,0,1]))
55/39: move_seed(np.array([1,1,2,0,0,1]))
55/40: move_seed(np.array([1,1,2,0,0,1]))
55/41: [move_seed(np.array([1,1,2,0,0,1])) for i in range(100)]
55/42: 4 is None
55/43: 4 != 4
55/44: %cpaste -q
56/44:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
56/46: %cpaste -q
57/46:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
57/48: %pip install matplotlib
57/49: %cpaste -q
58/49:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
58/51: %pip install seaborn
58/52: %cpaste -q
59/52:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
60/1: %cpaste -q
61/1:
# Standard imports
import sys
project_path = "/home/nick/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/bayes_opt/"))
import vacc
import numpy as np
import pandas as pd
61/3: %cpaste -q
62/3:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
62/5: %cpaste -q
63/5:
#tSIR parameters
params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))
tsir_config = {
    "iters":75,
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":3
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
63/7: %cpaste -q
64/7:
#%%
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
# optimization parameters
opt_config = {
    'obj':"attacksize",
    'V_repr':"max_ratio",
    'constraint_bnd':0.05,
    "attacksize_cutoff":1000
}
64/9: %cpaste -q
65/9:
#%%
import multiprocess
65/11: %cpaste -q
66/11:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
66/13: %cpaste -q
67/13:
with multiprocess.Pool(7) as p:
    engine.query(V_prime=engine.V_0,pool=p,n_sim=50)
    #result = engine.query(V_prime,pool=p,n_sim=50)
67/15: engine
67/16: engine.eval_history
67/17: vacc_df
67/18: %cpaste -q
68/18:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
68/20: top_5_alt_seed
68/21: %cpaste -q
69/21:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
69/23: %cpaste -q
70/23:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
70/25: %paste -q
71/25:
import numpy as np
import warnings
import dill
import pandas as pd
# TODO Need to import spatial tSIR module. is there a better way to do this??
import sys
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/"))
from spatial_tsir import *
"""
Write a wrapper for the stochastic tSIR that takes in different representations
of the vaccination vector and runs the simulation.
"""
class VaccRateOptEngine:
    """
    Wrapper class that should make it easier to 
    query the spatial tSIR model in an optimization problem.
    It serves as the "oracle" that you can query during the optimization routine.
    """
    # def __init__(self,
    #         V_0,inf_seed,
    #         sim_config, pop, distances,
    #         obj,V_repr,
    #         aux_param):
    def __init__(self,
            opt_config, V_0, seed,
            sim_config, pop, distances):
        # TODO: fill out the rest of the docstring
        """
        Create a new optimization oracle, initializing with certain fixed parameters.

        Note (as is the case with the other routines in this project) that the labeling
        of 'V_0', 'seed','pop', and 'distances' are implicitly assumed to be the same
        (e.g row N of pop refers to the same locality as does entry N of V_0 and so on).

        Args:
            opt_config (dict):
                Dictionary containing the parameters governing the behavior 
                of the objective function.
                Mandatory keys (must be specified):
                - 'obj': str
                    Specifies which summary statistic ('objective') will be computed from
                    the simulation and be used as the target of the objective function.
                    Possible values: (TODO)
                - 'V_repr': str
                    Specifies the format in which the vaccination vector V will be
                    represented.
                    Possible values:
                - 'constraint_bnd': float
                    Specifies the constraint that gets used to constrain the decision
                    space of the optimization problem. Broadly speaking, the constraint
                    will be of the form |V_0 - V'| < c where c is the constant specified
                    by 'constraint_bnd.'
            V_0 (numpy.ndarray):
                Vector of length (# of patches).
                The initial vaccination vector. Is used in the computation of the constraint.
            seed (numpy.ndarray):
                Vector of length (# of patches).
                The initial seeding strategy vector.
            sim_config (dict):
                Used to specify the parameters of the tSIR simulation.
            pop (Pandas.DataFrame):
                DataFrame. Has (# of patches) rows, and at least the column
                'patch_id' - names of the locations
                'pop' - population size of that location
            distances (numpy.ndarray):
                2D matrix of dimension (# of patches) x (# of patches).
                It specifies the distances between patches.
        """
        # store constants for simulation runs
        # store values relevant to optimization routine
        # that stay fixed over the duration of the optimization routine
        # TODO: validate parameters (e.g S+I+R = pop)?
        # TODO: objective with auxiliary parameters (i.e attacksize prob needs to specify the cutoff)?
        # input validation - these are essential
        # TODO: type checking too?
        assert 'obj' in opt_config.keys()
        assert 'V_repr' in opt_config.keys()
        assert 'constraint_bnd' in opt_config.keys()
        if opt_config['obj'] in ['attacksize_prob']: 
            # if you've specified the CDF based objective
            # you need to specify the cutoff quantity
            assert 'attacksize_cutoff' in opt_config.keys()

        assert opt_config['V_repr'] in ["ratio","max_ratio","raw"], "invalid V_repr (vaccine vector representation) argument passed"
        assert opt_config['obj'] in ["attacksize","peak","attacksize_prob"], "invalid objective function name passed"

        # save arguments as attributes of the object
        self.opt_config = opt_config
        self.sim_params = sim_config
        self.V_0 = V_0
        self.seed = seed
        self.pop_df = pop
        self.distances = distances

        # precompute vector forms of population, max to avoid recomputing it later
        self._pop_vec = np.array(self.pop_df['pop']) 
        self._max_pop = max(self._pop_vec)
        # store (input vector, objective) pairs
        self.eval_history = {'input':None,'output':None}
    def check_constraint(self,V_prime):
        # derivations for these constraints were in overleaf doc 'measles_optimization'
        #P = np.array(self.sim_params['pop']['pop']) # index 'pop' column from dataframe
        P = self._pop_vec
        V_0 = self.V_0
        V_repr = self.opt_config['V_repr']
        constraint_bnd = self.opt_config['constraint_bnd']
        if V_repr == "ratio":
            # check the basic domain constraint
            in_domain = (0 < V_prime).all() and (V_prime < 1).all()
            result_num = ((V_0 - V_prime) @ P)/np.linalg.norm(P,ord=1)
        elif V_repr == "max_ratio":
            in_domain = (0 < V_prime).all() and (V_prime < 1).all()
            result_num = (np.linalg.norm(P,ord=np.inf)/np.linalg.norm(P,ord=1))*\
                    np.abs(np.linalg.norm(V_0,ord=1) - np.linalg.norm(V_prime,ord=1))
        elif V_repr == "raw":
            in_domain = (0 < V_prime).all()
            result_num = np.linalg.norm(V_0-V_prime,ord=1) < constraint_bnd
        else:
            print("No constraint function exists for the V_repr type you've passed")
            result = -np.inf
        print(result_num) # print the constraint value
        result = (result_num < constraint_bnd) and in_domain
        return result
    def query(self,V_prime=None,seed_prime=None,
            multithread=True,pool=None,n_sim=None):
        if (type(V_prime) == type(None) and type(seed_prime) is type(None)) or\
                (type(V_prime) != type(None) and type(seed_prime) != type(None)):
            print("Please provide either V_prime or seed_prime")

        if type(seed_prime) != type(None):
            V_prime = self.V_0
            eval_mode = "V"
        elif type(V_prime) != type(None):
            # default to computation with the passed V_0
            seed_prime = self.seed
            eval_mode = "seed"
        else:
            print("bad argumetns") # TODO:
            return

        passed = self.check_constraint(V_prime)
        if not passed:
            print("Constraint violated")
            return np.array([-1]) # TODO: probably bad behavior, but ok for placeholding?

        # SETUP INITIAL STATE #
        if self.opt_config['V_repr'] == "max_ratio":
            V_unscaled = self._max_pop*V_prime
        elif self.opt_config['V_repr'] == "ratio":
            V_unscaled = self._pop_vec*V_prime # element by element
        elif self.opt_config['V_repr'] == "raw":
            pass
        else:
            raise ValueError("Invalid string for V_repr")
        V_unscaled = np.round(V_unscaled)
        initial_state = np.zeros((len(self.pop_df.index),2))
        #initial_state[:,0] = self.seed
        initial_state[:,0] = seed_prime
        initial_state[:,1] = V_unscaled

        if not multithread:
            # TODO singlethread evaluation
            pass
        assert type(pool) != type(None) and type(n_sim) != type(None),\
                "You must pass a multithread.Pool object and n_sim when in multithreading mode"
        sim_pool = spatial_tSIR_pool(
                    config = self.sim_params,
                    patch_pop = self.pop_df,
                    initial_state = initial_state,
                    n_sim = n_sim,
                    distances = self.distances
                )
        sim_pool.run_simulation(pool=pool)

        # return a sample of the statistic
        if self.opt_config['obj']=="attacksize":
            result = sim_pool.get_attack_size_samples()
        elif self.opt_config['obj']=="peak":
            result = np.max(sim_pool.get_samples(),axis=1)
        elif self.opt_config['obj']=="attacksize_prob":
            # It seems more natural to formulate it as
            # 1 if exceeded, 0 if below the bound
            # so the probability is probability of attack size above this cutoff?
            result = np.int64(sim_pool.get_attack_size_samples() > self.opt_config['attacksize_cutoff'])

        # keep a record of the evaluation results
        if type(self.eval_history['input']) == type(None) and type(self.eval_history['output']) == type(None):
            if eval_mode == "V":
                self.eval_history['input'] = np.array([V_prime])
            elif eval_mode = "seed":
                self.eval_history['input'] = np.array([seed_prime])
            self.eval_history['output'] = np.array([result])
        else:
            if eval_mode == "V":
                self.eval_history['input']= np.concatenate([self.eval_history['input'],np.array([V_prime])],axis=0)
            elif eval_mode = "seed":
            self.eval_history['output']= np.concatenate([self.eval_history['output'],np.array([result])],axis=0)
        return result
    def save_eval_history(self,
            path=None,
            as_csv=False,as_serial=True):
        # handle exceptional cases
        if path is None:
            raise ValueError("A path must be supplied.")
        if (self.eval_history['input'] is None) and (self.eval_history['output'] is None):
            print("No data to write.")
            return
        if as_serial:
            dill.dump(self.eval_history,file=out_file)
        elif as_csv:
            input_shape = np.shape(self.eval_history['input'])
            joined = np.concatenate([self.eval_history['input'],self.eval_history['output']],axis=1)
            joined = pd.DataFrame(joined)
            joined.rename(columns={0:'input',input_shape[1]:'output'})
            joined.to_csv(file=path,index=False)
        else:
            print("no valid save format specified")
71/27: %paste -q
72/27:
import numpy as np
import warnings
import dill
import pandas as pd
# TODO Need to import spatial tSIR module. is there a better way to do this??
import sys
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/"))
from spatial_tsir import *
"""
Write a wrapper for the stochastic tSIR that takes in different representations
of the vaccination vector and runs the simulation.
"""
class VaccRateOptEngine:
    """
    Wrapper class that should make it easier to 
    query the spatial tSIR model in an optimization problem.
    It serves as the "oracle" that you can query during the optimization routine.
    """
    # def __init__(self,
    #         V_0,inf_seed,
    #         sim_config, pop, distances,
    #         obj,V_repr,
    #         aux_param):
    def __init__(self,
            opt_config, V_0, seed,
            sim_config, pop, distances):
        # TODO: fill out the rest of the docstring
        """
        Create a new optimization oracle, initializing with certain fixed parameters.

        Note (as is the case with the other routines in this project) that the labeling
        of 'V_0', 'seed','pop', and 'distances' are implicitly assumed to be the same
        (e.g row N of pop refers to the same locality as does entry N of V_0 and so on).

        Args:
            opt_config (dict):
                Dictionary containing the parameters governing the behavior 
                of the objective function.
                Mandatory keys (must be specified):
                - 'obj': str
                    Specifies which summary statistic ('objective') will be computed from
                    the simulation and be used as the target of the objective function.
                    Possible values: (TODO)
                - 'V_repr': str
                    Specifies the format in which the vaccination vector V will be
                    represented.
                    Possible values:
                - 'constraint_bnd': float
                    Specifies the constraint that gets used to constrain the decision
                    space of the optimization problem. Broadly speaking, the constraint
                    will be of the form |V_0 - V'| < c where c is the constant specified
                    by 'constraint_bnd.'
            V_0 (numpy.ndarray):
                Vector of length (# of patches).
                The initial vaccination vector. Is used in the computation of the constraint.
            seed (numpy.ndarray):
                Vector of length (# of patches).
                The initial seeding strategy vector.
            sim_config (dict):
                Used to specify the parameters of the tSIR simulation.
            pop (Pandas.DataFrame):
                DataFrame. Has (# of patches) rows, and at least the column
                'patch_id' - names of the locations
                'pop' - population size of that location
            distances (numpy.ndarray):
                2D matrix of dimension (# of patches) x (# of patches).
                It specifies the distances between patches.
        """
        # store constants for simulation runs
        # store values relevant to optimization routine
        # that stay fixed over the duration of the optimization routine
        # TODO: validate parameters (e.g S+I+R = pop)?
        # TODO: objective with auxiliary parameters (i.e attacksize prob needs to specify the cutoff)?
        # input validation - these are essential
        # TODO: type checking too?
        assert 'obj' in opt_config.keys()
        assert 'V_repr' in opt_config.keys()
        assert 'constraint_bnd' in opt_config.keys()
        if opt_config['obj'] in ['attacksize_prob']: 
            # if you've specified the CDF based objective
            # you need to specify the cutoff quantity
            assert 'attacksize_cutoff' in opt_config.keys()

        assert opt_config['V_repr'] in ["ratio","max_ratio","raw"], "invalid V_repr (vaccine vector representation) argument passed"
        assert opt_config['obj'] in ["attacksize","peak","attacksize_prob"], "invalid objective function name passed"

        # save arguments as attributes of the object
        self.opt_config = opt_config
        self.sim_params = sim_config
        self.V_0 = V_0
        self.seed = seed
        self.pop_df = pop
        self.distances = distances

        # precompute vector forms of population, max to avoid recomputing it later
        self._pop_vec = np.array(self.pop_df['pop']) 
        self._max_pop = max(self._pop_vec)
        # store (input vector, objective) pairs
        self.eval_history = {'input':None,'output':None}
    def check_constraint(self,V_prime):
        # derivations for these constraints were in overleaf doc 'measles_optimization'
        #P = np.array(self.sim_params['pop']['pop']) # index 'pop' column from dataframe
        P = self._pop_vec
        V_0 = self.V_0
        V_repr = self.opt_config['V_repr']
        constraint_bnd = self.opt_config['constraint_bnd']
        if V_repr == "ratio":
            # check the basic domain constraint
            in_domain = (0 < V_prime).all() and (V_prime < 1).all()
            result_num = ((V_0 - V_prime) @ P)/np.linalg.norm(P,ord=1)
        elif V_repr == "max_ratio":
            in_domain = (0 < V_prime).all() and (V_prime < 1).all()
            result_num = (np.linalg.norm(P,ord=np.inf)/np.linalg.norm(P,ord=1))*\
                    np.abs(np.linalg.norm(V_0,ord=1) - np.linalg.norm(V_prime,ord=1))
        elif V_repr == "raw":
            in_domain = (0 < V_prime).all()
            result_num = np.linalg.norm(V_0-V_prime,ord=1) < constraint_bnd
        else:
            print("No constraint function exists for the V_repr type you've passed")
            result = -np.inf
        print(result_num) # print the constraint value
        result = (result_num < constraint_bnd) and in_domain
        return result
    def query(self,V_prime=None,seed_prime=None,
            multithread=True,pool=None,n_sim=None):
        if (type(V_prime) == type(None) and type(seed_prime) is type(None)) or\
                (type(V_prime) != type(None) and type(seed_prime) != type(None)):
            print("Please provide either V_prime or seed_prime")

        if type(seed_prime) != type(None):
            V_prime = self.V_0
            eval_mode = "V"
        elif type(V_prime) != type(None):
            # default to computation with the passed V_0
            seed_prime = self.seed
            eval_mode = "seed"
        else:
            print("bad argumetns") # TODO:
            return

        passed = self.check_constraint(V_prime)
        if not passed:
            print("Constraint violated")
            return np.array([-1]) # TODO: probably bad behavior, but ok for placeholding?

        # SETUP INITIAL STATE #
        if self.opt_config['V_repr'] == "max_ratio":
            V_unscaled = self._max_pop*V_prime
        elif self.opt_config['V_repr'] == "ratio":
            V_unscaled = self._pop_vec*V_prime # element by element
        elif self.opt_config['V_repr'] == "raw":
            pass
        else:
            raise ValueError("Invalid string for V_repr")
        V_unscaled = np.round(V_unscaled)
        initial_state = np.zeros((len(self.pop_df.index),2))
        #initial_state[:,0] = self.seed
        initial_state[:,0] = seed_prime
        initial_state[:,1] = V_unscaled

        if not multithread:
            # TODO singlethread evaluation
            pass
        assert type(pool) != type(None) and type(n_sim) != type(None),\
                "You must pass a multithread.Pool object and n_sim when in multithreading mode"
        sim_pool = spatial_tSIR_pool(
                    config = self.sim_params,
                    patch_pop = self.pop_df,
                    initial_state = initial_state,
                    n_sim = n_sim,
                    distances = self.distances
                )
        sim_pool.run_simulation(pool=pool)

        # return a sample of the statistic
        if self.opt_config['obj']=="attacksize":
            result = sim_pool.get_attack_size_samples()
        elif self.opt_config['obj']=="peak":
            result = np.max(sim_pool.get_samples(),axis=1)
        elif self.opt_config['obj']=="attacksize_prob":
            # It seems more natural to formulate it as
            # 1 if exceeded, 0 if below the bound
            # so the probability is probability of attack size above this cutoff?
            result = np.int64(sim_pool.get_attack_size_samples() > self.opt_config['attacksize_cutoff'])

        # keep a record of the evaluation results
        if type(self.eval_history['input']) == type(None) and type(self.eval_history['output']) == type(None):
            if eval_mode == "V":
                self.eval_history['input'] = np.array([V_prime])
            elif eval_mode == "seed":
                self.eval_history['input'] = np.array([seed_prime])
            self.eval_history['output'] = np.array([result])
        else:
            if eval_mode == "V":
                self.eval_history['input']= np.concatenate([self.eval_history['input'],np.array([V_prime])],axis=0)
            elif eval_mode == "seed":
            self.eval_history['output']= np.concatenate([self.eval_history['output'],np.array([result])],axis=0)
        return result
    def save_eval_history(self,
            path=None,
            as_csv=False,as_serial=True):
        # handle exceptional cases
        if path is None:
            raise ValueError("A path must be supplied.")
        if (self.eval_history['input'] is None) and (self.eval_history['output'] is None):
            print("No data to write.")
            return
        if as_serial:
            dill.dump(self.eval_history,file=out_file)
        elif as_csv:
            input_shape = np.shape(self.eval_history['input'])
            joined = np.concatenate([self.eval_history['input'],self.eval_history['output']],axis=1)
            joined = pd.DataFrame(joined)
            joined.rename(columns={0:'input',input_shape[1]:'output'})
            joined.to_csv(file=path,index=False)
        else:
            print("no valid save format specified")
72/29: %paste -q
73/29:
import numpy as np
import warnings
import dill
import pandas as pd
# TODO Need to import spatial tSIR module. is there a better way to do this??
import sys
project_path = "/home/nicholasw/Documents/sync/UVA files/Summer 2022 (C4GC with BII)/measles_metapop/{}"
sys.path.append(project_path.format("scripts/"))
from spatial_tsir import *
"""
Write a wrapper for the stochastic tSIR that takes in different representations
of the vaccination vector and runs the simulation.
"""
class VaccRateOptEngine:
    """
    Wrapper class that should make it easier to 
    query the spatial tSIR model in an optimization problem.
    It serves as the "oracle" that you can query during the optimization routine.
    """
    # def __init__(self,
    #         V_0,inf_seed,
    #         sim_config, pop, distances,
    #         obj,V_repr,
    #         aux_param):
    def __init__(self,
            opt_config, V_0, seed,
            sim_config, pop, distances):
        # TODO: fill out the rest of the docstring
        """
        Create a new optimization oracle, initializing with certain fixed parameters.

        Note (as is the case with the other routines in this project) that the labeling
        of 'V_0', 'seed','pop', and 'distances' are implicitly assumed to be the same
        (e.g row N of pop refers to the same locality as does entry N of V_0 and so on).

        Args:
            opt_config (dict):
                Dictionary containing the parameters governing the behavior 
                of the objective function.
                Mandatory keys (must be specified):
                - 'obj': str
                    Specifies which summary statistic ('objective') will be computed from
                    the simulation and be used as the target of the objective function.
                    Possible values: (TODO)
                - 'V_repr': str
                    Specifies the format in which the vaccination vector V will be
                    represented.
                    Possible values:
                - 'constraint_bnd': float
                    Specifies the constraint that gets used to constrain the decision
                    space of the optimization problem. Broadly speaking, the constraint
                    will be of the form |V_0 - V'| < c where c is the constant specified
                    by 'constraint_bnd.'
            V_0 (numpy.ndarray):
                Vector of length (# of patches).
                The initial vaccination vector. Is used in the computation of the constraint.
            seed (numpy.ndarray):
                Vector of length (# of patches).
                The initial seeding strategy vector.
            sim_config (dict):
                Used to specify the parameters of the tSIR simulation.
            pop (Pandas.DataFrame):
                DataFrame. Has (# of patches) rows, and at least the column
                'patch_id' - names of the locations
                'pop' - population size of that location
            distances (numpy.ndarray):
                2D matrix of dimension (# of patches) x (# of patches).
                It specifies the distances between patches.
        """
        # store constants for simulation runs
        # store values relevant to optimization routine
        # that stay fixed over the duration of the optimization routine
        # TODO: validate parameters (e.g S+I+R = pop)?
        # TODO: objective with auxiliary parameters (i.e attacksize prob needs to specify the cutoff)?
        # input validation - these are essential
        # TODO: type checking too?
        assert 'obj' in opt_config.keys()
        assert 'V_repr' in opt_config.keys()
        assert 'constraint_bnd' in opt_config.keys()
        if opt_config['obj'] in ['attacksize_prob']: 
            # if you've specified the CDF based objective
            # you need to specify the cutoff quantity
            assert 'attacksize_cutoff' in opt_config.keys()

        assert opt_config['V_repr'] in ["ratio","max_ratio","raw"], "invalid V_repr (vaccine vector representation) argument passed"
        assert opt_config['obj'] in ["attacksize","peak","attacksize_prob"], "invalid objective function name passed"

        # save arguments as attributes of the object
        self.opt_config = opt_config
        self.sim_params = sim_config
        self.V_0 = V_0
        self.seed = seed
        self.pop_df = pop
        self.distances = distances

        # precompute vector forms of population, max to avoid recomputing it later
        self._pop_vec = np.array(self.pop_df['pop']) 
        self._max_pop = max(self._pop_vec)
        # store (input vector, objective) pairs
        self.eval_history = {'input':None,'output':None}
    def check_constraint(self,V_prime):
        # derivations for these constraints were in overleaf doc 'measles_optimization'
        #P = np.array(self.sim_params['pop']['pop']) # index 'pop' column from dataframe
        P = self._pop_vec
        V_0 = self.V_0
        V_repr = self.opt_config['V_repr']
        constraint_bnd = self.opt_config['constraint_bnd']
        if V_repr == "ratio":
            # check the basic domain constraint
            in_domain = (0 < V_prime).all() and (V_prime < 1).all()
            result_num = ((V_0 - V_prime) @ P)/np.linalg.norm(P,ord=1)
        elif V_repr == "max_ratio":
            in_domain = (0 < V_prime).all() and (V_prime < 1).all()
            result_num = (np.linalg.norm(P,ord=np.inf)/np.linalg.norm(P,ord=1))*\
                    np.abs(np.linalg.norm(V_0,ord=1) - np.linalg.norm(V_prime,ord=1))
        elif V_repr == "raw":
            in_domain = (0 < V_prime).all()
            result_num = np.linalg.norm(V_0-V_prime,ord=1) < constraint_bnd
        else:
            print("No constraint function exists for the V_repr type you've passed")
            result = -np.inf
        print(result_num) # print the constraint value
        result = (result_num < constraint_bnd) and in_domain
        return result
    def query(self,V_prime=None,seed_prime=None,
            multithread=True,pool=None,n_sim=None):
        if (type(V_prime) == type(None) and type(seed_prime) is type(None)) or\
                (type(V_prime) != type(None) and type(seed_prime) != type(None)):
            print("Please provide either V_prime or seed_prime")

        if type(seed_prime) != type(None):
            V_prime = self.V_0
            eval_mode = "V"
        elif type(V_prime) != type(None):
            # default to computation with the passed V_0
            seed_prime = self.seed
            eval_mode = "seed"
        else:
            print("bad argumetns") # TODO:
            return

        passed = self.check_constraint(V_prime)
        if not passed:
            print("Constraint violated")
            return np.array([-1]) # TODO: probably bad behavior, but ok for placeholding?

        # SETUP INITIAL STATE #
        if self.opt_config['V_repr'] == "max_ratio":
            V_unscaled = self._max_pop*V_prime
        elif self.opt_config['V_repr'] == "ratio":
            V_unscaled = self._pop_vec*V_prime # element by element
        elif self.opt_config['V_repr'] == "raw":
            pass
        else:
            raise ValueError("Invalid string for V_repr")
        V_unscaled = np.round(V_unscaled)
        initial_state = np.zeros((len(self.pop_df.index),2))
        #initial_state[:,0] = self.seed
        initial_state[:,0] = seed_prime
        initial_state[:,1] = V_unscaled

        if not multithread:
            # TODO singlethread evaluation
            pass
        assert type(pool) != type(None) and type(n_sim) != type(None),\
                "You must pass a multithread.Pool object and n_sim when in multithreading mode"
        sim_pool = spatial_tSIR_pool(
                    config = self.sim_params,
                    patch_pop = self.pop_df,
                    initial_state = initial_state,
                    n_sim = n_sim,
                    distances = self.distances
                )
        sim_pool.run_simulation(pool=pool)

        # return a sample of the statistic
        if self.opt_config['obj']=="attacksize":
            result = sim_pool.get_attack_size_samples()
        elif self.opt_config['obj']=="peak":
            result = np.max(sim_pool.get_samples(),axis=1)
        elif self.opt_config['obj']=="attacksize_prob":
            # It seems more natural to formulate it as
            # 1 if exceeded, 0 if below the bound
            # so the probability is probability of attack size above this cutoff?
            result = np.int64(sim_pool.get_attack_size_samples() > self.opt_config['attacksize_cutoff'])

        # keep a record of the evaluation results
        if type(self.eval_history['input']) == type(None) and type(self.eval_history['output']) == type(None):
            if eval_mode == "V":
                self.eval_history['input'] = np.array([V_prime])
            elif eval_mode == "seed":
                self.eval_history['input'] = np.array([seed_prime])
            self.eval_history['output'] = np.array([result])
        else:
            if eval_mode == "V":
                self.eval_history['input']= np.concatenate([self.eval_history['input'],np.array([V_prime])],axis=0)
            elif eval_mode == "seed":
                self.eval_history['output']= np.concatenate([self.eval_history['output'],np.array([result])],axis=0)
        return result
    def save_eval_history(self,
            path=None,
            as_csv=False,as_serial=True):
        # handle exceptional cases
        if path is None:
            raise ValueError("A path must be supplied.")
        if (self.eval_history['input'] is None) and (self.eval_history['output'] is None):
            print("No data to write.")
            return
        if as_serial:
            dill.dump(self.eval_history,file=out_file)
        elif as_csv:
            input_shape = np.shape(self.eval_history['input'])
            joined = np.concatenate([self.eval_history['input'],self.eval_history['output']],axis=1)
            joined = pd.DataFrame(joined)
            joined.rename(columns={0:'input',input_shape[1]:'output'})
            joined.to_csv(file=path,index=False)
        else:
            print("no valid save format specified")
73/31: %cpaste -q
74/31:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
74/33: reload
74/34: import reload
74/35: import importlib
74/36: importlib.reload(vacc)
74/37: vacc = importlib.reload(vacc)
74/38: %cpaste -q
75/38:
#%%
import multiprocess
75/40: %cpaste -q
76/40:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
76/42: %cpaste -q
77/42:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
77/44: engine
77/45: engine.eval_history
77/46: vacc = importlib.reload(vacc)
77/47: %cpaste -q
78/47:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
78/49: %cpaste -q
79/49:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
79/51: engine.eval_history
79/52: engine.eval_history
79/53: vacc = importlib.reload(vacc)
79/54: %cpaste -q
80/54:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
80/56: %cpaste -q
81/56:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
81/58: vacc = importlib.reload(vacc)
81/59: %cpaste -q
82/59:
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
82/61: %cpaste -q
83/61:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=alt_seed,pool=p,n_sim=50)
83/63: engine
83/64: engine.eval_history
83/65: %cpaste -q
84/65:
#%%
top_5_alt_seed = vacc_df.sort_values(by='zipcode',ascending=False).head(5)
alt_seed = np.zeros(len(vacc_df.index))
np.put(alt_seed,top_5_alt_seed.index,1)
with multiprocess.Pool(7) as p:
    engine.query(seed_prime=engine.seed,pool=p,n_sim=50)
84/67: engine.eval_history
84/68: engine.eval_history['output']
84/69: np.mean(engine.eval_history['output'],axis=0)
84/70: np.mean(engine.eval_history['output'],axis=1)
84/71: engine.query()
84/72: engine.query
84/73:
class Hi:
        def __init__(self,quantity):
                    self.quantity = quantity
                        def dothething(self,x):
                                    return x+self.quantity
84/74:
class Hi:
        def __init__(self,quantity):
                    self.quantity = quantity
                        def dothething(self,x):
                                    return x+self.quantity
84/75: %paste -q
85/75:
class Hi:
    def __init__(self,quantity):
        self.quantity = quantity    
    def dothething(self,x):
        return x+self.quantity

def doit(func,x):
    return func(x)
85/77: h = Hi(4)
85/78: h.dothething(1)
85/79: doit(h.dothething)
85/80: doit(h.dothething,1)
85/81: from functools import partial
85/82: %cpaste -q
86/82:
#%%
import sim_anneal
86/84: sim_anneal
86/85: I
86/86: sim_anneal(I,10,100,move_seed,engine)
86/87: move_seed
86/88: %cpaste -q
87/88:
def move_seed(S,budget=None):
    if budget is None:
        budget = np.sum(S)
    nonzero_indices = np.nonzero(S)[0]
    nonzero_vals = S[nonzero_indices]
    rand_select = randint.rvs(0,budget)
    # what index does that correspond to?
    # first index where rolling sum > rand_select
    rolling_sum = np.cumsum(nonzero_vals)
    print(np.argmax(rolling_sum>rand_select))
    print(nonzero_indices)
    index_to_subtract = nonzero_indices[np.argmax(rolling_sum>rand_select)]
    index_to_add = randint.rvs(0,len(S)-1)
    to_return = S
    to_return[index_to_subtract] = to_return[index_to_subtract]-1
    to_return[index_to_add] = to_return[index_to_add]+1
    return to_return
87/90: sim_anneal(I,10,100,move_seed,engine)
87/91: sim_anneal.sim_anneal(I,10,100,move_seed,engine)
87/92: sim_anneal = reload(sim_anneal)
87/93: from importlib import reload
87/94: sim_anneal = reload(sim_anneal)
87/95: sim_anneal.sim_anneal(I,10,100,sim_anneal.move_seed,engine)
87/96: sim_anneal = reload(sim_anneal)
87/97: sim_anneal.sim_anneal(I,10,100,move_seed,engine)
87/98: %cpaste -q
88/98:
import numpy as np
import multiprocess
from scipy.stats import uniform, randint
from functools import partial
88/100: sim_anneal.sim_anneal(I,10,100,move_seed,engine)
90/1:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/2:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from functools import reduce
90/3:
low_b = 2
up_b = 5
90/4:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/5:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/6:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/7:
points_tr_df = pd.DataFrame(points_tr)
plt.scatter(x=points_tr_df[0],y=points_tr_df[2])
90/8: points_tr_df[0]
90/9:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
    x = [sums[i]-sums[i-1] for i in range(dim,1,-1)]
    x.reverse()
    x.insert(0,sums[0])
    return x
90/10: points_tr
90/11: points_tr[0]
90/12: inv_tr(points_tr[0],l,u)
90/13:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim,1,-1)]
    x.reverse()
    x.insert(0,sums[0])
    return x
90/14: inv_tr(points_tr[0],l,u)
90/15:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    x.insert(0,sums[0])
    return x
90/16: inv_tr(points_tr[0],l,u)
90/17: points[0]
90/18: [inv_tr(point,l,u) for point in points_tr]
90/19: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/20: [np.isclose(p1,p2) for p1,p2 in zip(points_un_tr,points)]
90/21: [all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)]
90/22: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/23:
n_points = 10000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(705)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/24:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(705)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/25:
# better way of generating random numbers?
n_points = 10
points = []
l=3
u=5
while len(points) < n_points:
    x = [np.random.random()*(u-l) + l for i in range(705)]
    x.insert(0,l)
    x.append(u)
    x.sort()
    points.append(np.diff(x))
90/26: points
90/27: points[0]
90/28: sum(points[0])
90/29: sum(points[1])
90/30:
# better way of generating random numbers?
n_points = 10
points = []
l=3
u=5
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
90/31: sum(points[1])
90/32: sum(points[0])
90/33: sum(points[1])
90/34: [sum(p) for p in points]
90/35:
# better way of generating random numbers?
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
90/36: [sum(p) for p in points]
90/37: [l<sum(p)<u for p in points]
90/38: all([l<sum(p)<u for p in points])
90/39: from multiprocess import Pool
90/40: [tr(p) for p in points]
90/41:
def tr(point,l=3,u=5,dim=705):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/42: [tr(p) for p in points]
90/43: points_tr = [tr(p) for p in points]
90/44: all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/45: [all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr]
90/46: points_tr
90/47: points_tr[0]
90/48: l<points_tr[0]
90/49: points_tr[0]
90/50: l<=points_tr[0]
90/51: l<=points_tr[0] and points_tr[0] <= u
90/52: [all(l<=p) and all(p <= u) for p in points_tr]
90/53: all([all(l<=p) and all(p <= u) for p in points_tr])
90/54: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/55: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/56: points_un_tr
90/57:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    x.insert(0,sums[0])
    return np.array(x)
90/58: points_un_tr
90/59: points_un_tr[0]
90/60: sum(points_un_tr[0])
90/61: points_un_tr[0]
90/62: len(points_un_tr[0])
90/63: points[0
90/64: points[0]
90/65: len(points[0])
90/66:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/67:
def tr(point,l=3,u=5,dim=705):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/68:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/69: all([all(l<=p) and all(p <= u) for p in points_tr])
90/70: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/71: len(points_un_tr[0])
90/72: len(points[0])
90/73: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/74: points_un_tr[0]
90/75: points_un_tr[0]-points[0]
90/76: sum(points_un_tr[0])
90/77:
def tr(point,l=3,u=5,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/78:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/79:
def tr(point,l=3,u=5,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/80:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/81: all([all(l<=p) and all(p <= u) for p in points_tr])
90/82: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/83: len(points_un_tr[0])
90/84: sum(points_un_tr[0])
90/85: points_un_tr
90/86: points_un_tr[0] - points[0]
90/87: points[0]
90/88: points_un_tr[0]
90/89:
# better way of generating random numbers?
n_points = 1000
points = []
l=400
u=600
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
90/90: all([l<sum(p)<u for p in points])
90/91:
def tr(point,l=3,u=5,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/92:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/93: all([all(l<=p) and all(p <= u) for p in points_tr])
90/94: points_tr[0]
90/95:
def tr(point,l=400,u=600,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/96:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/97: points_tr[0]
90/98:
# better way of generating random numbers?
n_points = 1000
points = []
l=400
u=600
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
90/99: all([l<sum(p)<u for p in points])
90/100:
def tr(point,l=400,u=600,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/101:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/102: points_tr[0]
90/103: all([all(l<=p) and all(p <= u) for p in points_tr])
90/104: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/105: len(points_un_tr[0])
90/106: len(points[0])
90/107: sum(points_un_tr[0])
90/108: points_un_tr[0]
90/109: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/110: len(points_un_tr[0])
90/111: sum(points_un_tr[0])
90/112: points_un_tr[0]
90/113: points[0]
90/114: points_un_tr[1:-1]
90/115: points_un_tr[1:]
90/116: points_un_tr[2:]
90/117: points_un_tr[0][1:]
90/118: points_un_tr[0][1:] - points[0][:-1]
90/119: points_un_tr[0][1:] - points[0][:-1]
90/120:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
90/121: all([l<sum(p)<u for p in points])
90/122:
def tr(point,l=400,u=600,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/123:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/124: points_tr[0]
90/125: all([all(l<=p) and all(p <= u) for p in points_tr])
90/126:
def tr(point,l=10000,u=12000,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/127:
points_tr = [tr(p) for p in points]
len(points_tr[0])
90/128: points_tr[0]
90/129: all([all(l<=p) and all(p <= u) for p in points_tr])
90/130: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/131: points_un_tr[0][1:] - points[0][:-1]
90/132: points_un_tr[0][1:]
90/133: points_un_tr[0][1:10]
90/134: points[0][1:10]
90/135: points_un_tr[0][0:10]
90/136: points_un_tr[0][1:10]
90/137: points[0][0:10]
90/138:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    x.insert(0,sums[1])
    return np.array(x)
90/139:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    #x.insert(0,sums[1])
    return np.array(x)
90/140: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/141: points_un_tr[0][1:10]
90/142: points[0][0:10]
90/143: points_un_tr[0][0:10]
90/144: len(points_un_tr[0])
90/145: len(points[0])
90/146: points_un_tr[0][700:-1]
90/147: points[0][700:-1]
90/148:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    x.append(0,sums[1])
    return np.array(x)
90/149: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/150:
# does the inverse transformation work?
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    x.append(sums[1])
    return np.array(x)
90/151: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/152: points_un_tr[0][700:-1]
90/153: points[0][700:-1]
90/154:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/155: all([l<sum(p)<u for p in points])
90/156:
def tr(point,l=10000,u=12000,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/157:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/158: points_tr[0]
90/159: np.type(points_tr[0])
90/160: type(points_tr[0])
90/161: type(points_tr[0][0])
90/162:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/163: all([all(l<=p) and all(p <= u) for p in points_tr])
90/164: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/165: len(points_un_tr[0])
90/166: len(points[0])
90/167: sum(points_un_tr[0])
90/168: points_un_tr[0][700:-1]
90/169: points[0][700:-1]
90/170: points_un_tr[0] - points[0]
90/171: (points_un_tr[0] - points[0])/points[0]
90/172: (points_un_tr[0] - points[0])*100/points[0]
90/173: (points_un_tr[1] - points[1])*100/points[0]
90/174: points[1][700:-1]
90/175: points_un_tr[1][700:-1]
90/176: points[2][700:-1]
90/177: points_un_tr[2][700:-1]
90/178: points[3][700:-1]
90/179: points_un_tr[3][700:-1]
90/180:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i+1,i] = (l-point[i])/(u-l)
    print(coeff_matrix)
90/181:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/182:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/183:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/184:
points_tr_df = pd.DataFrame(points_tr)
plt.scatter(x=points_tr_df[0],y=points_tr_df[2])
90/185: points_tr_df[0]
90/186: inv_tr_lin_solve(point_tr[0],l,u)
90/187: inv_tr_lin_solve(points_tr[0],l,u)
90/188:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
90/189: inv_tr_lin_solve(points_tr[0],l,u)
90/190:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    x = np.solve(coeff_matrix,np.array(point))
    return x
90/191: inv_tr_lin_solve(points_tr[0],l,u)
90/192:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    x = np.linalg.solve(coeff_matrix,np.array(point))
    return x
90/193: inv_tr_lin_solve(points_tr[0],l,u)
90/194: points[0]
90/195: inv_tr(points_tr[0],l,u)
90/196: points[0]
90/197:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/198:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/199:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/200:
points_tr_df = pd.DataFrame(points_tr)
plt.scatter(x=points_tr_df[0],y=points_tr_df[2])
90/201: points_tr_df[0]
90/202:
# does the inverse transformation work?
# it does, but it's quite numerically unstable
def inv_tr(point,l,u):
    dim = len(point)
    cur_index = dim-1
    sums = []
    sums.insert(0,point[cur_index])
    while cur_index > 0:
        last_sum = sums[0]
        next_sum = (last_sum*(point[cur_index-1]-l))/(u-l)
        sums.insert(0,next_sum)
        cur_index = cur_index -1
    x = [sums[i]-sums[i-1] for i in range(dim-1,0,-1)]
    x.reverse()
    x.append(sums[1])
    return np.array(x)
90/203:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    x = np.linalg.solve(coeff_matrix,np.array(point))
    return x
90/204: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/205: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/206: inv_tr(points_tr[0],l,u)
90/207: points[0]
90/208: inv_tr_lin_solve(points_tr[0],l,u)
90/209:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    x = np.linalg.solve(coeff_matrix,np.array(point))
    return x
90/210: inv_tr_lin_solve(points_tr[0],l,u)
90/211:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[-1] = point[-1]
    x = np.linalg.solve(coeff_matrix,np.array(point))
    return x
90/212: inv_tr_lin_solve(points_tr[0],l,u)
90/213:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[-1] = point[-1]
    print(b)
    x = np.linalg.solve(coeff_matrix,b)
    return x
90/214: inv_tr_lin_solve(points_tr[0],l,u)
90/215: points[0]
90/216: inv_tr(points_tr[0],l,u)
90/217: inv_tr_lin_solve(points_tr[0],l,u)
90/218:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i+1])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[-1] = point[-1]
    print(b)
    x = np.linalg.solve(coeff_matrix,b)
    return x
90/219: inv_tr(points_tr[0],l,u)
90/220: inv_tr_lin_solve(points_tr[0],l,u)
90/221: points[0]
90/222:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i-1])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[-1] = point[-1]
    print(b)
    x = np.linalg.solve(coeff_matrix,b)
    return x
90/223: inv_tr_lin_solve(points_tr[0],l,u)
90/224:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[-1] = point[-1]
    print(b)
    x = np.linalg.solve(coeff_matrix,b)
    return x
90/225: inv_tr_lin_solve(points_tr[0],l,u)
90/226:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    coeff_matrix[dim-1,:] = np.ones(dim)
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    print(b)
    x = np.linalg.solve(coeff_matrix,b)
    return x
90/227: inv_tr_lin_solve(points_tr[0],l,u)
90/228:
sol = inv_tr_lin_solve(points_tr[0],l,u)
sum(sol)
90/229:
sol = inv_tr_lin_solve(points_tr[0],l,u)
sol
sum(sol)
90/230:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/231:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    print(b)
    x = np.linalg.solve(coeff_matrix,b)
    return x
90/232: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/233: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/234: inv_tr(points_tr[0],l,u)
90/235:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
91/1: import numpy as np
91/2: np.ones((5,5))
91/3: np.tril(np.ones((5,5)))
90/236:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    print(b)
    x = np.linalg.solve(coeff_matrix@np.tril(np.ones((dim,dim)),b))
    return x
90/237:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/238:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    print(b)
    L = np.tril(np.ones((dim,dim)))
    x = np.linalg.solve(coeff_matrix@L,b)
    return x
90/239:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/240:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    print(b)
    L = np.tril(np.ones((dim,dim)))
    x = np.linalg.solve(coeff_matrix@L,b)
    return x
90/241:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/242:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/243: all([l<sum(p)<u for p in points])
90/244:
def tr(point,l=10000,u=12000,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/245:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/246: all([all(l<=p) and all(p <= u) for p in points_tr])
90/247: points_un_tr = [inv_tr_lin_solve(point,l,u) for point in points_tr]
90/248:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = np.linalg.solve(coeff_matrix@L,b)
    return x
90/249: points_un_tr = [inv_tr_lin_solve(point,l,u) for point in points_tr]
90/250: points_un_tr[3][700:-1]
90/251: points[3][700:-1]
90/252: len(points_un_tr[0])
90/253: len(points[0])
90/254: sum(points_un_tr[0])
90/255: (points_un_tr[1] - points[1])*100/points[0]
90/256: points_un_tr[3][0:10]
90/257: points[3][0:10]
90/258: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/259:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/260:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/261:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/262:
points_tr_df = pd.DataFrame(points_tr)
plt.scatter(x=points_tr_df[0],y=points_tr_df[2])
90/263: points_tr_df[0]
90/264: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/265: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/266: inv_tr(points_tr[0],l,u)
90/267:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/268: points[0]
90/269: points[0]
90/270:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/271: points[0]
90/272: sol-points[0]
90/273:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from functools import reduce
import scipy
90/274:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
90/275: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
90/276: all([all(np.isclose(p1,p2)) for p1,p2 in zip(points_un_tr,points)])
90/277: inv_tr(points_tr[0],l,u)
90/278:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/279: points[0]
90/280: sol-points[0]
90/281:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(705)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/282:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/283: all([l<sum(p)<u for p in points])
90/284:
def tr(point,l=10000,u=12000,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/285:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/286: points_un_tr = [inv_tr_lin_solve(point,l,u) for point in points_tr]
90/287: inv_tr_lin_solve(points_tr[0],l,u)[0:10]
90/288: points[3][0:10]
90/289: points[0][0:10]
90/290:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    #x = scipy.linalg.solve(coeff_matrix@L,b)
    def objective(x):
        return (coeff_matrix@L)@x-b
    return root(objective,np.ones(dim))
90/291:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/292:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/293:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/294:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
90/295:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x)
sum(sol)
90/296: points[0]
90/297:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x)
sum(sol.x)
90/298:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/299:
def tr(point,l=10000,u=12000,dim=706):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/300:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(705)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/301: all([l<sum(p)<u for p in points])
90/302: points
90/303: points[0]
90/304: len(points[0])
90/305:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(704)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/306: len(points[0])
90/307:
def tr(point,l=10000,u=12000,dim=705):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/308:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/309: inv_tr_lin_solve(points_tr[0],l,u)[0:10]
90/310:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x)
90/311:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x[0:10])
90/312: points[0][0:10]
90/313:
#sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x[1:10])
90/314: points[0][0:10]
90/315:
#sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x[1:11])
90/316: print(sol.x[1:11])-points[0][0:10]
90/317: sol.x[1:11]-points[0][0:10]
90/318: (sol.x[1:11]-points[0][0:10])/points[0][0:10]
90/319: (sol.x[1:11]-points[0][0:10])*100/points[0][0:10]
90/320:
def tr(point,l=10000,u=12000,dim=705):
    point_tr = [((np.sum(point[0:i-1])*(u-l))/np.sum(point[0:i])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/321:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/322: all([all(l<=p) and all(p <= u) for p in points_tr])
90/323:
def tr(point,l=10000,u=12000,dim=705):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(0,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/324:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/325:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/326: points_tr[1]
90/327: tr(points[1])
90/328:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
90/329:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
90/330:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
90/331:
points_tr_df = pd.DataFrame(points_tr)
plt.scatter(x=points_tr_df[0],y=points_tr_df[2])
90/332: points_tr[1]
90/333: tr(points[1])
90/334: tr(points[1],dim=3)
90/335: tr(points[1],l=l,u=u,dim=3)
90/336:
def tr(point,l=10000,u=12000,dim=705):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(1,dim-1)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/337: tr(points[1],l=l,u=u,dim=3)
90/338:
def tr(point,l=10000,u=12000,dim=705):
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(1,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/339: points_tr[1]
90/340: tr(points[1],l=l,u=u,dim=3)
90/341:
def tr(point,l=10000,u=12000,dim=705):
    point_tr = [np.float128(((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1]))) + l for i in range(1,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/342: points_tr[1]
90/343: tr(points[1],l=l,u=u,dim=3)
90/344:
def tr(point,l=10000,u=12000,dim=705):
    point = np.float128(np.array(point))
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(1,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/345: tr(points[1],l=l,u=u,dim=3)
90/346: points_tr[1]
90/347: tr(points[1],l=l,u=u,dim=3)
90/348: tr(points[1],l=l,u=u,dim=3)[0]
90/349:
# better way of generating random numbers?
n_points = 1000
points = []
l=10000
u=12000
while len(points) < n_points:
    target_sum = np.random.random()*(u-l) + l
    x = [np.random.random() for i in range(704)]
    x.insert(0,0)
    x.append(1)
    x.sort()
    points.append(np.diff(x)*target_sum)
    
points = np.float128(points)
90/350: all([l<sum(p)<u for p in points])
90/351: len(points[0])
90/352:
def tr(point,l=10000,u=12000,dim=705):
    point = np.float128(np.array(point))
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(1,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
90/353:
points_tr = np.array([tr(p) for p in points])
len(points_tr[0])
90/354: points_un_tr = [inv_tr_lin_solve(point,l,u) for point in points_tr]
90/355:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x[1:11])
90/356:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x[0:10])
90/357: points[0][0:10]
90/358: (sol.x - points[0])*100/points[0]
90/359:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
90/360:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x[0:10])
90/361:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol[0:10])
90/362: (sol.x - points[0])*100/points[0]
90/363: (sol- points[0])*100/points[0]
90/364: (sol-points[0][0:10])*100/points[0][0:10]
90/365: (sol- points[0])*100/points[0]
90/366:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol[0:10])
90/367: points[0][0:10]
92/1:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
92/2:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from functools import reduce
import scipy
92/3:
n_points = 1000
points = []
l=3
u=5
while len(points) < n_points:
    candidate = [np.random.random()*5 for i in range(3)]
    if l <= sum(candidate) <= u:
        points.append(candidate)
    else:
        continue
92/4:
points_tr =\
[
    (
        x[0]*(u-l)/(x[0]+x[1])+l,
        (x[0]+x[1])*(u-l)/(x[0]+x[1]+x[2]) + l,
        x[0]+x[1]+x[2]
    )
    for x in points
]
92/5:
# yes, the constraints are satisfied
# This is necessary but is it sufficient?
all([all(l<np.array(p)) and all(np.array(p)<=u) for p in points_tr])
92/6:
points_tr_df = pd.DataFrame(points_tr)
plt.scatter(x=points_tr_df[0],y=points_tr_df[2])
92/7: tr(points[1],l=l,u=u,dim=3)[0]
92/8: points_tr[1]
92/9:
def tr(point,l=10000,u=12000,dim=705):
    point = np.float128(np.array(point))
    point_tr = [((np.sum(point[0:i])*(u-l))/np.sum(point[0:i+1])) + l for i in range(1,dim)]
    point_tr.append(np.sum(point))
    return np.array(point_tr)
92/10: points_un_tr = [inv_tr(point,l,u) for point in points_tr]
92/11: sol-points[0]
92/12:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x)
sum(sol.x)
92/13:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve(point,l,u):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = point[dim-1]
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
92/14:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol.x)
sum(sol.x)
92/15:
sol = inv_tr_lin_solve(points_tr[0],l,u)
print(sol)
sum(sol)
92/16: points[0]
92/17: sol-points[0]
92/18:
def generate_number_summing_to(sum_target,dim,n_points)
    # better way of generating random numbers?
    points = []
    while len(points) < n_points:
        x = [np.random.random() for i in range(dim)]
        x.insert(0,0)
        x.append(1)
        x.sort()
        points.append(np.diff(x)*sum_target)
    points = np.float128(points)
    return points
92/19:
def generate_number_summing_to(sum_target,dim,n_points):
    # better way of generating random numbers?
    points = []
    while len(points) < n_points:
        x = [np.random.random() for i in range(dim)]
        x.insert(0,0)
        x.append(1)
        x.sort()
        points.append(np.diff(x)*sum_target)
    points = np.float128(points)
    return points
92/20: generate_number_summing_to(5,4,100)
92/21: generate_number_summing_to(160000,4,100)
92/22: np.sum(generate_number_summing_to(160000,4,100),axis=1)
92/23: points = generate_number_summing_to(160000,4,100)
92/24:
def generate_points_between(l,u,dim,n_points):
    points = []
    while len(points) < n_points:
        x = [np.random.random()*(u-l) + l for i in range(dim)]
    points = np.float128(points)
    return points
92/25: points = generate_points_between(10000,12000,4,100)
92/26:
def generate_points_between(l,u,dim,n_points):
    points = []
    while len(points) < n_points:
        x = [np.random.random()*(u-l) + l for i in range(dim)]
        points.append(x)
    points = np.float128(points)
    return points
92/27: points = generate_points_between(10000,12000,4,100)
92/28: points
92/29:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
92/30: inv_tr_lin_solve_eq(points[0],l=10000.u=12000,c=5)
92/31: inv_tr_lin_solve_eq(points[0],l=10000,u=12000,c=5)
92/32: sum(inv_tr_lin_solve_eq(points[0],l=10000,u=12000,c=5))
92/33: inv_tr_lin_solve_eq(points[0],l=10000,u=12000,c=5)
95/1:
# load libraries
import numpy as np
import pandas as pd
95/2:
# load and clean data
with open("project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/3:
# load and clean data
with open("project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/4:
# load and clean data
with open("../project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/5:
# load and clean data
with open("../project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
    
print(project_path)
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/6:
# load and clean data
with open("../project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
    
print(project_path)
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/7:
# load and clean data
with open("../project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
    
print(project_path)
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/8:
# load and clean data
with open("../project_dir.txt") as f:
    project_path = f.read().strip() + "{}"
    
print(project_path)
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]

top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
95/9: constraint_bnd = 0.05
95/10: constraint_bnd = 0.05
95/11: vacc_df
95/12:
constraint_bnd = 0.05
V_0 = (vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop'])
95/13: V_0
95/14:
constraint_bnd = 0.05
V_0 = np.array((vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop']))
95/15: V_0
95/16: # now let's generate some candidate vectors in our new space
95/17:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 0
u = 100
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim)]) for i in range(n_points)])
95/18:
constraint_bnd = 0.05
V_0 = np.array((vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop']))
dim = len(V_0)
95/19:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 0
u = 100
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim)]) for i in range(n_points)])
95/20: points
95/21:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 0
u = 100
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim)]) for i in range(n_points)])
95/22: points
95/23: points[0]
95/24:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/25:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 0
u = 100
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
95/26: inv_tr_lin_solve_eq(points[0],l,u,c=constraint_bnd*sum(P))
95/27:
constraint_bnd = 0.05
V_0 = np.array((vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop']))
dim = len(V_0)

eq_constraint_bnd = constraint_bnd*sum(vacc_df['pop'])
95/28: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/29:
# load libraries
import numpy as np
import pandas as pd
import scipy
95/30: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/31: len(inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd))
95/32: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/33:
constraint_bnd = 0.05
V_0 = np.array((vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop']))
dim = len(V_0)
print(dim)
eq_constraint_bnd = constraint_bnd*sum(vacc_df['pop'])
95/34:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim+1)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim+1)
    b[dim] = c
    #print(b)
    L = np.tril(np.ones((dim+1,dim+1)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/35: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/36:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 0
u = 100
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points)
95/37:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 0
u = 100
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/38: len(inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd))
95/39: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/40: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)*vacc_df['pop']
95/41: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/42: sum(inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd))
95/43:
sum(inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd))
eq_constraint_bnd
95/44:
sum(inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd))
print(eq_constraint_bnd)
95/45:
sum(inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd))
eq_constraint_bnd
95/46: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
94/1:
def generate_number_summing_to(sum_target,dim,n_points):
    # better way of generating random numbers?
    points = []
    while len(points) < n_points:
        x = [np.random.random() for i in range(dim)]
        x.insert(0,0)
        x.append(1)
        x.sort()
        points.append(np.diff(x)*sum_target)
    points = np.float128(points)
    return points
94/2:
def generate_number_summing_to(sum_target,dim,n_points):
    # better way of generating random numbers?
    points = []
    while len(points) < n_points:
        x = [np.random.random() for i in range(dim)]
        x.insert(0,0)
        x.append(1)
        x.sort()
        points.append(np.diff(x)*sum_target)
    points = np.float128(points)
    return points
94/3:
def generate_points_between(l,u,dim,n_points):
    points = []
    while len(points) < n_points:
        x = [np.random.random()*(u-l) + l for i in range(dim)]
        points.append(x)
    points = np.float128(points)
    return points
94/4: points = generate_number_summing_to(160000,4,100)
95/47:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/48: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/49:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/50: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/51:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/52:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/53:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/54: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/55:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
dim=10
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/56:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/57: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/58:
constraint_bnd = 0.05
V_0 = np.array((vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop']))
dim = len(V_0)
print(dim)
eq_constraint_bnd = constraint_bnd*sum(vacc_df['pop'])
95/59:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/60:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim-1):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim)
    b[dim-1] = c
    #print(b)
    L = np.tril(np.ones((dim,dim)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/61: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/62: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd*1000)
95/63: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/64:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim+1)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim+1)
    b[dim] = c
    #print(b)
    L = np.tril(np.ones((dim+1,dim+1)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/65: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/66:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
dim=10
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/67:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim+1)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim+1)
    b[dim] = c
    #print(b)
    L = np.tril(np.ones((dim+1,dim+1)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/68: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/69:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/70:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim+1)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim+1)
    b[dim] = c
    #print(b)
    L = np.tril(np.ones((dim+1,dim+1)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/71: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/72:
constraint_bnd = 0.05
V_0 = np.array((vacc_df['pop'] - vacc_df['nVaccCount'])/(vacc_df['pop']))
dim = len(V_0)
print(dim)
eq_constraint_bnd = constraint_bnd*sum(vacc_df['pop'])
95/73:
# now let's generate some candidate vectors in our new space

# todo: what to choose for lower and upper bounds? idk
l = 10
u = 110
n_points = 100
points = np.array([np.array([np.random.random()*(u-l)+l for i in range(dim-1)]) for i in range(n_points)])
len(points[0])
95/74:
# maybe we can make it better by posing it as a linear system
# and hoping that the np solver does something smart
from scipy.optimize import root
def inv_tr_lin_solve_eq(point,l,u,c):
    dim = len(point)
    coeff_matrix = np.eye(dim+1)
    # add bottom row of ones
    #coeff_matrix[dim-1,:] = np.ones(dim)
    # add coefficients to off-diagonal
    for i in range(0,dim):
        coeff_matrix[i,i+1] = (l-point[i])/(u-l)
    #print(coeff_matrix)
    b = np.zeros(dim+1)
    b[dim] = c
    #print(b)
    L = np.tril(np.ones((dim+1,dim+1)))
    x = scipy.linalg.solve(coeff_matrix@L,b)
    return x
    #def objective(x):
    #    return (coeff_matrix@L)@x-b
    #return root(objective,np.ones(dim))
95/75: inv_tr_lin_solve_eq(points[0],l,u,c=eq_constraint_bnd)
95/76:
# generate random numbers summing to the equality constraint
def generate_number_summing_to(sum_target,dim,n_points):
    # better way of generating random numbers?
    points = []
    while len(points) < n_points:
        x = [np.random.random() for i in range(dim)]
        x.insert(0,0)
        x.append(1)
        x.sort()
        points.append(np.diff(x)*sum_target)
    points = np.float128(points)
    return points
95/77: generate_number_summing_to(eq_constraint_bnd,dim,10)
95/78: generate_number_summing_to(eq_constraint_bnd,dim,1)
95/79: generate_number_summing_to(eq_constraint_bnd,dim,1)/vacc_df['pop']
95/80: generate_number_summing_to(eq_constraint_bnd,dim)
95/81: generate_number_summing_to(eq_constraint_bnd,dim,1)
95/82: generate_number_summing_to(eq_constraint_bnd,dim,1).T
95/83: generate_number_summing_to(eq_constraint_bnd,dim,1).T / vacc_df['pop']
95/84: generate_number_summing_to(eq_constraint_bnd,dim,1)[0]
95/85: generate_number_summing_to(eq_constraint_bnd,dim,1)[0]/vacc_df['pop']
95/86: generate_number_summing_to(eq_constraint_bnd,dim-1,1)[0]/vacc_df['pop']
95/87: max(generate_number_summing_to(eq_constraint_bnd,dim-1,1)[0]/vacc_df['pop'])
95/88: generate_number_summing_to(eq_constraint_bnd,dim-1,1)[0]/vacc_df['pop']
95/89:
def generate_v_delta_candidate(V_0):
    V_delta = np.zeros(len(V_0))
    for i,value in enumerate(V_0):
        V_delta[i] = np.random.random()*value
    return V_delta
95/90: generate_v_delta_candidate(V_0)
95/91:
# doing the change of variable
test_point = generate_v_delta_candidate(V_0)*np.array(vacc_df['pop'])
95/92: test_point
95/93: generate_v_delta_candidate@vacc_df['pop']
95/94:
# doing the change of variable
test_point = generate_v_delta_candidate(V_0)*np.array(vacc_df['pop'])
95/95: generate_v_delta_candidate@vacc_df['pop']
95/96: test_point
95/97:
def generate_v_delta_candidate(V_0):
    V_delta = np.zeros(len(V_0))
    for i,value in enumerate(V_0):
        V_delta[i] = np.random.random()*value
    return V_delta
95/98:
# doing the change of variable
test_point = generate_v_delta_candidate(V_0)*np.array(vacc_df['pop'])
95/99: test_point
95/100: sum(test_point)
95/101:
def generate_number_summing_to(sum_target,dim,n_points):
    # better way of generating random numbers?
    points = []
    while len(points) < n_points:
        x = [np.random.random() for i in range(dim)]
        x.insert(0,0)
        x.append(1)
        x.sort()
        points.append(np.diff(x)*sum_target)
    points = np.float128(points)
    return points
95/102: generate_number_summing_to(eq_constraint_bnd,dim,1)
95/103: generate_number_summing_to(eq_constraint_bnd,dim,1)[0]
95/104: generate_number_summing_to(eq_constraint_bnd,dim,1)[0]/vacc_df['pop']
95/105: generate_number_summing_to(eq_constraint_bnd,dim-1,1)[0]/vacc_df['pop']
100/1: import Simulation
101/1: import Simulation
101/2: import Simulation
101/3:
parser = argparse.ArgumentParser(description = '')
parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
args = parser.parse_args()

## Environment Settings ##
if args.contextdim:
    context_dimension = args.contextdim
else:
    context_dimension = 25

if args.actionset:
    actionset = args.actionset
else:
    actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/4:
import Simulation
from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
101/5:
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()


## Environment Settings ##
if args.contextdim:
    context_dimension = args.contextdim
else:
    context_dimension = 25

if args.actionset:
    actionset = args.actionset
else:
    actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/6:
import argparse
parser = argparse.ArgumentParser(description = '')
parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
args = parser.parse_args()

## Environment Settings ##
if args.contextdim:
    context_dimension = args.contextdim
else:
    context_dimension = 25

if args.actionset:
    actionset = args.actionset
else:
    actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/7:
import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

args = {}

## Environment Settings ##
if args.contextdim:
    context_dimension = args.contextdim
else:
    context_dimension = 25

if args.actionset:
    actionset = args.actionset
else:
    actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/8:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/9:
import Simulation
from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager
101/10:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/11:
from Simulation import *
from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager
101/12:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
101/13:
from Simulation import *
from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager
101/14:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
102/1:
from Simulation import *
from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager
102/2:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
102/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
simExperiment.runAlgorithms(algorithms)
102/4:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
102/5:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
103/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
103/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
104/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
104/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
104/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.1)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
104/4:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.9)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
104/5:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.5)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
104/6:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.5)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
105/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
105/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.5)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
105/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
106/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
106/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
106/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=5)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
106/4:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.5)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
106/5:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=np.sqrt(2))

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
107/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
107/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=np.sqrt(2))

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
107/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=1)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
107/4:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
108/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
108/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
109/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
109/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
110/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
110/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.05)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
110/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.1)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
110/4:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
110/5:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
110/6:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
110/7:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
110/8:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
110/9:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
111/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
111/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
112/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/4:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
#algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/5:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
#algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=10*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/6:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
#algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1000*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/7:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
#algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1e6*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
112/8:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
113/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
113/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_Var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1e6*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
113/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1e6*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
114/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
114/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1e6*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
115/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
115/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1e6*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
115/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms)
116/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
116/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.3)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
116/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.01,epsilon=0.3)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
116/4:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.3)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
116/5:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
116/6:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
116/7:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
116/8:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
118/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
118/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
119/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
119/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
120/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
120/2:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
120/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
121/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
121/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
122/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
122/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
122/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
123/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
123/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
123/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
123/4:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
124/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
124/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
125/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
125/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
126/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
126/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
127/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
127/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
127/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
128/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
128/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
129/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
129/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
130/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
130/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
131/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
131/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
132/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
132/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
134/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
134/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
134/3:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
135/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
135/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
135/3:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
136/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
136/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
136/3:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
136/4:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
136/5:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
136/6:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
136/7:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
136/8:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
136/9:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
136/10:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
136/11:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
137/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
137/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
138/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
138/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
139/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
139/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
139/3:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
139/4:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
140/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
140/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
141/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
141/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
141/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.01,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
141/4:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.5,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
141/5:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
141/6:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit_BayesLinear import LinTSBandit
142/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit_BayesLinear import LinTSBandit
142/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
143/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit_BayesLinear import LinTSBandit
143/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
144/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit_BayesLinear import LinTSBandit
144/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
145/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
145/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
#algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2,alpha=1)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=0.1,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
146/1:
import numpy as np
import matplotlib.pyplot as plt
146/2:
true_beta = np.array([5,10])
true_var = 2
data_x = np.linspace(-10,10,2000)
data_y = data_x@true_beta + np.random.normal(loc=0,scale=np.sqrt(true_var))
146/3:
true_beta = np.array([5,10])
true_var = 2
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var))
146/4: plt.scatter(data_x,data_y)
146/5:
true_beta = np.array([5,10])
true_var = 10
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var))
146/6: plt.scatter(data_x,data_y)
146/7:
true_beta = np.array([5,10])
true_var = 100
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var))
146/8: plt.scatter(data_x,data_y)
146/9:
import numpy as np
import matplotlib.pyplot as plt
146/10:
true_beta = np.array([5,10])
true_var = 100
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var))
146/11: plt.scatter(data_x,data_y)
146/12:
true_beta = np.array([1,3])
true_var = 100
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var))
146/13: plt.scatter(data_x,data_y)
146/14:
true_beta = np.array([1,3])
true_var = 100
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var),size=len(data_x))
146/15: plt.scatter(data_x,data_y)
146/16:
true_beta = np.array([1,3])
true_var = 10
data_x = np.linspace(-10,10,2000)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var),size=len(data_x))
146/17: plt.scatter(data_x,data_y)
146/18: plt.scatter(data_x,data_y,alpha=0.2)
146/19:
true_beta = np.array([1,3])
true_var = 10
n_points = 2000
data_x = np.linspace(-10,10,n_points)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var),size=n_points)
146/20:
import numpy as np
import matplotlib.pyplot as plt
import scipy
146/21: plt.scatter(data_x,data_y,alpha=0.2)
146/22:
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var
146/23:
true_beta = np.array([1,3])
true_var = 10
n_points = 2000
dim = 2
data_x = np.linspace(-10,10,n_points)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var),size=n_points)
146/24: plt.scatter(data_x,data_y,alpha=0.2)
146/25:
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var
146/26:
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var
print(mean_vec,cov_mat)
146/27:
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var
print(mean_vec)
print(cov_mat)
146/28:
t = 100
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var
print(mean_vec)
print(cov_mat)
146/29: prior_var = 10
146/30:
# all at once
X = np.array([np.array([x,1]) for x in data_x])
print(X)
#mean_vec = (1/prior_var * np.eye(dim)+)
146/31:
# all at once
X = np.array([np.array([x,1]) for x in data_x]).T
print(X)
#mean_vec = (1/prior_var * np.eye(dim)+)
146/32: X@y
146/33: y
146/34: data_y
146/35: X@data_y
146/36:
# all at once
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
146/37:
# all at once
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
print(mean_Vec)
146/38:
# all at once
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
print(mean_vec)
146/39:
# all at once
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
sigma_mat = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)*true_var
print(mean_vec)
146/40:
# all at once
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
sigma_mat = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)*true_var
print(mean_vec)
print(sigma_mat)
146/41: print(X@X.T)
146/42:
sum(np.outer(
    np.array([x,1]),
    np.array([x,1]))
    for x in data_x
   )
146/43:
t = 2000
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/44:
t = 1500
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/45:
t = 2000
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/46:
# and yes the iterative estimation works correctly too
t = 2000
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/47:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(X@mean_vec)
146/48: mean_vec
146/49: X.T@mean_vec
146/50:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(X.T@mean_vec)
146/51:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec)
146/52:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
146/53:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,X.T@mean_vec,color="red")
146/54:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
#plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,X.T@mean_vec,color="red")
146/55: cov_mat
146/56: X@cov_mat
146/57: X.T@cov_mat
146/58: X.T@cov_mat@X
146/59: X[1,0]
146/60: X[1,:]
146/61: X[:,1]
146/62: X[:,1]@cov_mat
146/63: X[:,1]@cov_mat@X[:,1]
146/64: [X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]
146/65: [1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]
146/66: np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])
146/67:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),color="red")
146/68: X.T@mean_vec=np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])
146/69: X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])
146/70:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),color="blue")
146/71: X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])
146/72:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
146/73: X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])
146/74: X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+true_var
146/75:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+true_var,
         color="red")
146/76:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+1.95*true_var,
         color="red")
plt.plot(data_x,
         X.T@mean_vec - np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]) - 1.95*true_var,
         color="blue")
146/77:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+1.95*true_var,
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]) - 1.95*true_var,
         color="blue")
146/78:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+true_var,
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]) - 1.95*true_var,
         color="blue")
146/79:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+true_var,
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]) - true_var,
         color="blue")
146/80:
# and yes the iterative estimation works correctly too
t = 1500
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/81:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+true_var,
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]) - true_var,
         color="blue")
146/82:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/83:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/84:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/85:
# and yes the iterative estimation works correctly too
t = 5
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*true_var

print(mean_vec)
print(cov_mat)
146/86:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/87:
# and yes the iterative estimation works correctly too
t = 5
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(1/true_var)

print(mean_vec)
print(cov_mat)
146/88:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/89:
# and yes the iterative estimation works correctly too
t = 5
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/90:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/91:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/92:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/93:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/94: prior_var = 1000
146/95:
# all at once: yes, these estimators work
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
sigma_mat = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)*true_var
print(mean_vec)
print(sigma_mat)
146/96: print(X@X.T)
146/97:
# yes, the identity is correct
sum(np.outer(
    np.array([x,1]),
    np.array([x,1]))
    for x in data_x
   )
146/98:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/99:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/100: prior_var = 10e6
146/101:
# all at once: yes, these estimators work
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
sigma_mat = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)*true_var
print(mean_vec)
print(sigma_mat)
146/102: print(X@X.T)
146/103:
# yes, the identity is correct
sum(np.outer(
    np.array([x,1]),
    np.array([x,1]))
    for x in data_x
   )
146/104:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/105:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/106:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/107:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/108:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 100

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/109:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/110:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 150

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/111:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/112:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/113:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/114:
# and yes the iterative estimation works correctly too
t = 10000
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/115:
# and yes the iterative estimation works correctly too
t = 2000
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/116:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/117:
# and yes the iterative estimation works correctly too
t = 1500
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/118:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/119: X.T@mean_vec+np.array([1.95*X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)])+true_var
146/120: cov_mat
146/121: X[:,0]
146/122: X[:,0]@cov_mat
146/123: X[:,0]@cov_mat@[X:,0]
146/124: X[:,0]@cov_mat@X[:,0]
146/125: 2*np.sqrt(X[:,0]@cov_mat@X[:,0])
146/126:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array(np.sqrt([X[:,1]@cov_mat@X[:,1]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/127:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,1]@cov_mat@X[:,1]) for i in range(0,n_points)])
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/128:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,1]@cov_mat@X[:,1]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([X[:,1]@cov_mat@X[:,1] for i in range(0,n_points)]),
         color="blue")
146/129:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/130:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/131:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/132:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/133:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/134:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/135:
# and yes the iterative estimation works correctly too
t = 100
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/136:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/137:
# and yes the iterative estimation works correctly too
t = 1000
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None
for i in range(t):
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/138:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/139:
# and yes the iterative estimation works correctly too
t = 100
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/140:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/141:
# and yes the iterative estimation works correctly too
t = 10
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/142:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/143:
# and yes the iterative estimation works correctly too
t = 5
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/144:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/145:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 10

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/146:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
146/147:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1])
146/148:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="orange")
146/149:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="orange",style="dotted")
146/150:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="orange",linestyle="dotted")
146/151: matplotlib.rcParams['figure.figsize'] = [width, height]
146/152: matplotlib.rcParams['figure.figsize'] = [10, 7]
146/153:
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import scipy
matplotlib.rcParams['figure.figsize'] = [10, 7]
146/154:
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import scipy
matplotlib.rcParams['figure.figsize'] = [10, 7]
146/155:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="orange",linestyle="dotted")
146/156:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="orange",linestyle="orange")
146/157:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="orange")
146/158:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.2)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/159:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/160:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 9

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/161:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 100

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/162:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/163:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 200

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/164:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/165:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/166:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/167:
true_beta = np.array([1,3])
true_var = 100
n_points = 2000
dim = 2
data_x = np.linspace(-10,10,n_points)
data_y = data_x*true_beta[0] + true_beta[1] + np.random.normal(loc=0,scale=np.sqrt(true_var),size=n_points)
146/168: plt.scatter(data_x,data_y,alpha=0.2)
146/169:
# all at once: yes, these estimators work
X = np.array([np.array([x,1]) for x in data_x]).T
mean_vec = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)@(X@data_y)
sigma_mat = np.linalg.inv(1/prior_var*np.eye(dim)+X@X.T)*true_var
print(mean_vec)
print(sigma_mat)
146/170: print(X@X.T)
146/171:
# yes, the identity is correct
sum(np.outer(
    np.array([x,1]),
    np.array([x,1]))
    for x in data_x
   )
146/172:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/173:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/174:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/175:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
146/176:
# and yes the iterative estimation works correctly too
t = 7
prior_var = 1000

A = (1/prior_var)*np.eye(dim)
b = np.zeros(dim)
A_inv = prior_var*np.eye(dim)

mean_vec = None
cov_mat = None

data_sampled_index = np.random.choice(list(range(0,n_points)),size=t,replace=False)
for i in data_sampled_index:
    x = np.array([data_x[i],1])
    y = data_y[i]
    
    A += np.outer(x,x)
    b += y*x
    
    Ainv = np.linalg.inv(A)
    
    mean_vec = Ainv@b
    cov_mat = Ainv*(true_var)

print(mean_vec)
print(cov_mat)
146/177:
# plot prediction interval
plt.scatter(data_x,data_y,alpha=0.1)
plt.plot(data_x,X.T@mean_vec,color="red")
plt.plot(data_x,
         X.T@mean_vec + 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x,
         X.T@mean_vec - 2*np.array([np.sqrt(X[:,i]@cov_mat@X[:,i]) for i in range(0,n_points)]),
         color="blue")
plt.plot(data_x, data_x*true_beta[0] + true_beta[1],color="black")
148/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
148/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
148/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
150/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
150/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
151/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
151/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
152/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
152/2:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=2*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
152/3:
#import argparse
#parser = argparse.ArgumentParser(description = '')
#parser.add_argument('--contextdim', type=int, help='Set dimension of context features.')
#parser.add_argument('--actionset', type=str, help='Set dimension of context features.')
#args = parser.parse_args()

#args = {}

## Environment Settings ##
#if args.contextdim:
#    context_dimension = args.contextdim
#else:
context_dimension = 25
actionset = "basis_vector"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.1  # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}


#algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
#algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=100)
152/4:
def mp1_1_2(n_articles,NoiseScale):
    context_dimension = 25
    actionset = "basis_vector"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.1  # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}
    #algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
    #algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
    algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
    algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=100)
152/5: mp1_1_2(n_articles=25,NoiseScale=0.1)
152/6:
def mp1_1_2(n_articles,NoiseScale,print_every=1000):
    context_dimension = 25
    actionset = "basis_vector"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.1  # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}
    #algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
    #algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
    algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
    algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=print_every)
152/7: mp1_1_2(n_articles=25,NoiseScale=0.1)
152/8: mp1_1_2(n_articles=15,NoiseScale=0.1)
152/9: mp1_1_2(n_articles=25,NoiseScale=0.2)
152/10: mp1_1_2(n_articles=25,NoiseScale=0.3)
152/11: mp1_1_2(n_articles=25,NoiseScale=0.4)
152/12: mp1_1_2(n_articles=25,NoiseScale=0.5)
152/13:
def mp1_1_2(n_articles,NoiseScale,print_every=1000):
    context_dimension = 25
    actionset = "basis_vector"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.1  # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}
    #algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
    algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
    algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
    algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=print_every)
152/14: mp1_1_2(n_articles=25,NoiseScale=0.1)
152/15: mp1_1_2(n_articles=25,NoiseScale=0.2)
152/16: mp1_1_2(n_articles=25,NoiseScale=0.3)
152/17: mp1_1_2(n_articles=25,NoiseScale=0.4)
152/18: mp1_1_2(n_articles=25,NoiseScale=0.5)
152/19: mp1_1_2(n_articles=25,NoiseScale=0.5)
152/20: mp1_1_2(n_articles=15,NoiseScale=0.1)
152/21: mp1_1_2(n_articles=20,NoiseScale=0.1)
152/22: mp1_1_2(n_articles=25,NoiseScale=0.1)
152/23: mp1_1_2(n_articles=30,NoiseScale=0.1)
152/24: mp1_1_2(n_articles=35,NoiseScale=0.1)
152/25: mp1_1_2(n_articles=15,NoiseScale=0.3)
152/26: mp1_1_2(n_articles=20,NoiseScale=0.3)
152/27: mp1_1_2(n_articles=15,NoiseScale=0.3)
152/28: mp1_1_2(n_articles=20,NoiseScale=0.3)
152/29: mp1_1_2(n_articles=25,NoiseScale=0.3)
152/30: mp1_1_2(n_articles=30,NoiseScale=0.3)
152/31: mp1_1_2(n_articles=35,NoiseScale=0.3)
152/32: mp1_1_2(n_articles=40,NoiseScale=0.3)
152/33: mp1_1_2(n_articles=45,NoiseScale=0.3)
152/34: mp1_1_2(n_articles=50,NoiseScale=0.3)
149/1:
def mp1_prob2(context_dimension, NoiseScale, n_articles, actionset="random")
    #context_dimension = 25
    #actionset = "random"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.2 # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}

    #algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
    algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
    algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
149/2:
def mp1_prob2(context_dimension, NoiseScale, n_articles, actionset="random"):
    #context_dimension = 25
    #actionset = "random"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.2 # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}

    algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
    algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
    algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
152/35:
def mp1_1_2(context_dimension,NoiseScale,print_every=1000):
    context_dimension = 25
    actionset = "basis_vector"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.1  # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}
    #algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
    algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
    algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
    algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=print_every)
152/36: mp1_1_2(n_articles=15,NoiseScale=0.3)
152/37:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.UCBBandit import UCBBandit
from lib.TSBandit import TSBandit
152/38:
def mp1_1_2(n_articles=25,NoiseScale,print_every=1000):
    context_dimension = n_articles
    actionset = "basis_vector"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.1  # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}
    #algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
    algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
    algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
    algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=print_every)
152/39:
def mp1_1_2(n_articles=25,NoiseScale=0.1,print_every=1000):
    context_dimension = n_articles
    actionset = "basis_vector"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.1  # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}
    #algorithms['EpsilonGreedyLinearBandit'] = EpsilonGreedyLinearBandit(dimension=context_dimension, lambda_=0.1, epsilon=None)
    algorithms['EpsilonGreedyMultiArmedBandit'] = EpsilonGreedyMultiArmedBandit(num_arm=n_articles, epsilon=None)
    algorithms['UpperConfidenceBound'] = UCBBandit(num_arm=n_articles, delta=0.4, bandit_var=NoiseScale**2)
    algorithms['ThompsonSampling'] = TSBandit(num_arm=n_articles, bandit_var=NoiseScale**2, prior_var=1.5*NoiseScale**2)

    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=print_every)
152/40: mp1_1_2(n_articles=25,NoiseScale=0.1)
152/41: mp1_1_2(n_articles=25,NoiseScale=0.2)
152/42: mp1_1_2(n_articles=25,NoiseScale=0.3)
152/43: mp1_1_2(n_articles=25,NoiseScale=0.4)
152/44: mp1_1_2(n_articles=25,NoiseScale=0.5)
152/45: mp1_1_2(n_articles=15,NoiseScale=0.3)
152/46: mp1_1_2(n_articles=20,NoiseScale=0.3)
152/47: mp1_1_2(n_articles=25,NoiseScale=0.3)
152/48: mp1_1_2(n_articles=30,NoiseScale=0.3)
152/49: mp1_1_2(n_articles=35,NoiseScale=0.3)
152/50: mp1_1_2(n_articles=40,NoiseScale=0.3)
152/51: mp1_1_2(n_articles=45,NoiseScale=0.3)
152/52: mp1_1_2(n_articles=50,NoiseScale=0.3)
149/3:
def mp1_prob2(context_dimension=25, NoiseScale=0.2, n_articles=25, actionset="random"):
    #context_dimension = 25
    #actionset = "random"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.2 # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}

    algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
    algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
    algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
149/4: mp1_prob2(context_dimension=15)
149/5:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
153/1:
from Simulation import *
from conf import sim_files_folder, save_address
from util_functions import featureUniform, gaussianFeature
from Articles import ArticleManager
from Users import UserManager

from lib.EpsilonGreedyLinearBandit import EpsilonGreedyLinearBandit
from lib.EpsilonGreedyMultiArmedBandit import EpsilonGreedyMultiArmedBandit
from lib.LinUCBBandit import LinUCBBandit
from lib.LinTSBandit import LinTSBandit
153/2:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.2 # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

#algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
153/3:
context_dimension = 25
actionset = "random"  # "basis_vector" or "random"

testing_iterations = 2000
NoiseScale = 0.2 # standard deviation of Gaussian noise
n_articles = 25
n_users = 10
poolArticleSize = None

if actionset == "basis_vector":
    n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

## Set Up Simulation ##
UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
users = UM.simulateThetafromUsers()
AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
articles = AM.simulateArticlePool(actionset)

simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                    testing_iterations=testing_iterations,
                                    plot=True,
                                    articles=articles,
                                    users = users,
                                    noise=lambda: np.random.normal(scale=NoiseScale),
                                    signature=AM.signature,
                                    NoiseScale=NoiseScale,
                                    poolArticleSize=poolArticleSize)

## Initiate Bandit Algorithms ##
algorithms = {}

algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
## Run Simulation ##
print("Starting for ", simExperiment.simulation_signature)
regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
153/4:
def mp1_prob2(context_dimension=25, NoiseScale=0.2, n_articles=25, actionset="random"):
    #context_dimension = 25
    #actionset = "random"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.2 # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}

    algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
    algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
    algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
153/5: mp1_prob2(context_dimension=15)
153/6:
def mp1_prob2(context_dimension=25, NoiseScale=0.2, n_articles=25, actionset="random", no_epsilon=True):
    #context_dimension = 25
    #actionset = "random"  # "basis_vector" or "random"

    testing_iterations = 2000
    #NoiseScale = 0.2 # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}

    if not no_epsilon:
        algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
    algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
    algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
153/7: mp1_prob2(context_dimension=15)
153/8: mp1_prob2(context_dimension=25)
153/9: mp1_prob2(context_dimension=35)
153/10: mp1_prob2(NoiseScale=0.2)
153/11: mp1_prob2(NoiseScale=0.4)
153/12: mp1_prob2(NoiseScale=0.6)
153/13: mp1_prob2(NoiseScale=0.8)
153/14: mp1_prob2(n_articles=15)
153/15: mp1_prob2(n_articles=25)
153/16: mp1_prob2(n_articles=35)
153/17: mp1_prob2(n_articles=45)
153/18: mp1_prob2(actionset="basis_vector")
153/19: mp1_prob2(actionset="random")
153/20:
def mp1_prob2(context_dimension=25, NoiseScale=0.2, n_articles=25, actionset="random", no_epsilon=True,iterations=2000):
    #context_dimension = 25
    #actionset = "random"  # "basis_vector" or "random"

    testing_iterations = iterations
    #NoiseScale = 0.2 # standard deviation of Gaussian noise
    #n_articles = 25
    n_users = 10
    poolArticleSize = None

    if actionset == "basis_vector":
        n_articles = context_dimension  # there can be at most context_dimension number of basis vectors

    ## Set Up Simulation ##
    UM = UserManager(context_dimension, n_users, thetaFunc=gaussianFeature, argv={'l2_limit': 1})
    users = UM.simulateThetafromUsers()
    AM = ArticleManager(context_dimension, n_articles=n_articles, argv={'l2_limit':1})
    articles = AM.simulateArticlePool(actionset)

    simExperiment = simulateOnlineData( context_dimension=context_dimension,
                                        testing_iterations=testing_iterations,
                                        plot=True,
                                        articles=articles,
                                        users = users,
                                        noise=lambda: np.random.normal(scale=NoiseScale),
                                        signature=AM.signature,
                                        NoiseScale=NoiseScale,
                                        poolArticleSize=poolArticleSize)

    ## Initiate Bandit Algorithms ##
    algorithms = {}

    if not no_epsilon:
        algorithms["EpsilonGreedyLinearBandit"] = EpsilonGreedyLinearBandit(dimension=context_dimension,lambda_=0.1,epsilon=0.6)
    algorithms["LinUCB"] = LinUCBBandit(dimension=context_dimension, lambda_=0.21,bandit_var=NoiseScale**2, S=2,err_prob=1e-3)
    algorithms["LinTS"] = LinTSBandit(dimension=context_dimension, lambda_=10,bandit_var=NoiseScale**2)
    ## Run Simulation ##
    print("Starting for ", simExperiment.simulation_signature)
    regret = simExperiment.runAlgorithms(algorithms,print_every=1000)
153/21: mp1_prob2(actionset="basis_vector",iterations=4000)
153/22: mp1_prob2(actionset="random",iterations=4000)
154/1: %cpaste -q
155/1:
# Standard imports
import scripts.bayes_opt.vacc as vacc
import numpy as np
import pandas as pd
156/1: %cpaste -q
157/1:
# Standard imports
import scripts.bayes_opt.vacc as vacc
import numpy as np
import pandas as pd
157/3: %cpaste -q
158/3:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
158/5: pwd
158/6: project_path = pwd
158/7: project_path = Out[5]
158/8: %cpaste -q
159/8:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
159/10: project_path = Out[5]+"/{}"
159/11: %cpaste -q
160/11:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
160/13: %cpaste -q
161/13:
#tSIR parameters
params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))
tsir_config = {
    "iters":75,
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
161/15: %cpaste -q
162/15:
#%%
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
# optimization parameters
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
162/17: %cpaste -q
163/17:
#%%
import multiprocess
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
#V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
163/19: %cpaste -q
164/19:
with multiprocess.Pool(12) as p:
    engine.query(V_prime=engine.V_0,pool=p,n_sim=150)
    #engine.query
    #result = engine.query(V_prime,pool=p,n_sim=50)
164/21: %cpaste -q
165/21:
with multiprocess.Pool(12) as p:
    engine.query(V_delta=engine.V_0,pool=p,n_sim=150)
    #engine.query
    #result = engine.query(V_prime,pool=p,n_sim=50)
165/23: %cpaste -q
166/23:
with multiprocess.Pool(12) as p:
    engine.query(V_delta=engine.V_0,pool=p,n_sim=150)
    #engine.query
    #result = engine.query(V_prime,pool=p,n_sim=50)
166/25: import reload
166/26: restart
166/27: %restart
166/28: %reload
167/1: %cpaste -q
168/1:
# Standard imports
import scripts.bayes_opt.vacc as vacc
import numpy as np
import pandas as pd
168/3: pwd
168/4: project_path = Out[5]+"/{}"
168/5: project_path = Out[3]+"/{}"
168/6: %cpaste -q
169/6:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
169/8: %cpaste -q
170/8:
#tSIR parameters
params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))
tsir_config = {
    "iters":75,
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
170/10: %cpaste -q
171/10:
#%%
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
# optimization parameters
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
171/12: %cpaste -q
172/12:
#%%
import multiprocess
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
#V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
172/14: %cpaste -q
173/14:
with multiprocess.Pool(12) as p:
    engine.query(V_delta=engine.V_0,pool=p,n_sim=150)
    #engine.query
    #result = engine.query(V_prime,pool=p,n_sim=50)
174/1: %cpaste -q
175/1:
# Standard imports
import scripts.bayes_opt.vacc as vacc
import numpy as np
import pandas as pd
175/3: pwd
175/4: pwd+"/{}"
175/5: project_path = Out[3]+"/{}"
175/6: %cpaste -q
176/6:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
176/8: %cpaste -q
177/8:
#tSIR parameters
params = np.load(project_path.format("outputs/log_calib_grav_params_jul22.npy"))
tsir_config = {
    "iters":75,
    "tau1":params[0],
    "tau2":params[1],
    "rho":params[2],
    "theta":params[3],
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
177/10: %cpaste -q
178/10:
#%%
top_5 = vacc_df.sort_values(by='pop',ascending=False).head(5)
I = np.zeros(len(vacc_df.index))
np.put(I,top_5.index,1)
# optimization parameters
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
178/12: %cpaste -q
179/12:
#%%
import multiprocess
V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(vacc_df['pop'])
#V_0 = (vacc_df['pop']-vacc_df['nVaccCount'])/(max(vacc_df['pop']))
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=V_0, seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
V_prime = engine.V_0.copy()
V_prime[512] = V_prime[512]-0.8
179/14: %cpaste -q
180/14:
with multiprocess.Pool(12) as p:
    engine.query(V_delta=engine.V_0,pool=p,n_sim=150)
    #engine.query
    #result = engine.query(V_prime,pool=p,n_sim=50)
182/1: %cpaste -q
183/1:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
183/3: %cpaste -q
184/3:
vacc_df = {
        'pop': [120,120,150,200]
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,5,5,5],
        [5,0,5,5],
        [5,5,0,5],
        [5,5,5,0],
]
184/5: %cpaste -q
185/5:
vacc_df = {
        'pop': [120,120,150,200],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,5,5,5],
        [5,0,5,5],
        [5,5,0,5],
        [5,5,5,0],
]
185/7: %cpaste -q
186/7:
vacc_df = {
        'pop': [120,120,150,200],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,5,5,5],
        [5,0,5,5],
        [5,5,0,5],
        [5,5,5,0],
]
186/9: %cpaste -q
187/9:
tsir_config = {
    "iters":75,
    "tau1":1,
    "tau2":1,
    "rho":1,
    "theta":1,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
187/11: vacc_df
187/12: %cpaste -q
188/12:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
188/14: pwd
188/15: project_path = Out[14]+ '{}'
188/16:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
188/17: project_path = Out[14]+ '/{}'
188/18:
#%%
# vaccination data and population data from sifat
vacc_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_immunization_sifat.csv"))
# drop 0 population entries, they won't affect the simulation
vacc_df = vacc_df[vacc_df['population'] > 0].reset_index(drop=True)
vacc_df.rename({'population':'pop'},axis=1,inplace=True)
# load distance matrix computed from nominatim and geopy distance function
dist_df = pd.read_csv(project_path.format("data/VA_zipcodes_cleaned/ZC_distance_sifat_nom_geopy.csv"))
# need to replace 0's in distance matrix to avoid divide by zero in gravity formula
default_dist = 0.5
dist_df.loc[dist_df[np.isclose(dist_df['distKM'],0)].index,'distKM']=default_dist
# convert to matrix
dist_mat = dist_df.pivot(index='zipcode1',columns='zipcode2',values='distKM')
dist_mat = dist_mat.replace(np.nan,0)
# align matrix
dist_mat = dist_mat.loc[vacc_df['zipcode'],vacc_df['zipcode']]
188/19: vacc_Df
188/20: vacc_df
188/21: dist_mat
188/22: %cpaste -q
189/22:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [120,120,150,200],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,5,5,5],
        [5,0,5,5],
        [5,5,0,5],
        [5,5,5,0],
]
189/24: %cpaste -q
190/24:
tsir_config = {
    "iters":75,
    "tau1":1,
    "tau2":1,
    "rho":1,
    "theta":1,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
190/26: %cpaste -q
191/26:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
191/28: %cpaste -q
192/28:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
192/30: %cpaste -q
193/30:
with multiprocess.Pool(12) as p:
    engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150)
193/32: %cpaste -q
194/32:
with multiprocess.Pool(12) as p:
    result = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150)
194/34: result
195/1: %cpaste -q
196/1:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [120,120,150,200],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,5,5,5],
        [5,0,5,5],
        [5,5,0,5],
        [5,5,5,0],
]
196/3: %cpaste -q
197/3:
tsir_config = {
    "iters":75,
    "tau1":1,
    "tau2":1,
    "rho":1,
    "theta":1,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
197/5: %cpaste -q
198/5:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
198/7: %cpaste -q
199/7:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
199/9: %cpaste -q
200/9:
with multiprocess.Pool(12) as p:
    result = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150)
200/11: result
200/12: %cpaste -q
201/12:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
201/14: sim_pool
201/15: sim_pool.get_attack_size_samples()
201/16: sim_pool.plot_paths()
201/17: %matplotlib qt
201/18: sim_pool.plot_paths()
201/19: %cpaste -q
202/19:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [1200,1200,1500,2000],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,5,5,5],
        [5,0,5,5],
        [5,5,0,5],
        [5,5,5,0],
]
202/21: %cpaste -q
203/21:
tsir_config = {
    "iters":75,
    "tau1":1,
    "tau2":1,
    "rho":1,
    "theta":1,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
203/23: %cpaste -q
204/23:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
204/25: %cpaste -q
205/25:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
205/27: %cpaste -q
206/27:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
206/29: sim_pool.plot_paths()
206/30: %cpaste -q
207/30:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [1200,1200,1500,2000],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,1,5,5],
        [1,0,5,5],
        [5,5,0,10],
        [5,5,10,0],
]
207/32: %cpaste -q
208/32:
tsir_config = {
    "iters":75,
    "tau1":1,
    "tau2":1,
    "rho":1,
    "theta":1,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
208/34: %cpaste -q
209/34:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
209/36: %cpaste -q
210/36:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
210/38: %cpaste -q
211/38:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
211/40: sim_pool.plot_paths()
211/41: sim_pool.plot_paths()
211/42: %cpaste -q
212/42:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [1200,1200,1500,2000],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,1,5,5],
        [1,0,5,5],
        [5,5,0,10],
        [5,5,10,0],
]
212/44: %cpaste -q
213/44:
tsir_config = {
    "iters":75,
    "tau1":0.7,
    "tau2":1.2,
    "rho":.97,
    "theta":0.1,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
213/46: %cpaste -q
214/46:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
214/48: %cpaste -q
215/48:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
215/50: %cpaste -q
216/50:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
216/52: sim_pool.plot_paths()
216/53: %cpaste -q
217/53:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [1200,1200,1500,2000],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,1,5,5],
        [1,0,5,5],
        [5,5,0,10],
        [5,5,10,0],
]
217/55: %cpaste -q
218/55:
tsir_config = {
    "iters":75,
    "tau1":0.7,
    "tau2":1.2,
    "rho":.97,
    "theta":0.05,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
218/57: %cpaste -q
219/57:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
219/59: %cpaste -q
220/59:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
220/61: %cpaste -q
221/61:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
221/63: sim_pool.plot_paths()
221/64: result
221/65: sim_pool.plot_paths()
221/66: sim_pool.plot_paths()
221/67: %cpaste -q
222/67:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
vacc_df = {
        'pop': [1200,1200,1500,2000],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
dist_mat = [
        [0,1,5,5],
        [1,0,5,5],
        [5,5,0,10],
        [5,5,10,0],
]
222/69: %cpaste -q
223/69:
tsir_config = {
    "iters":75,
    "tau1":0.7,
    "tau2":1.2,
    "rho":.97,
    "theta":0.05,
    "alpha":0.97,
    "beta":7
}
sim_params = {
        'config':tsir_config,
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
223/71: %cpaste -q
224/71:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"peak",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
224/73: %cpaste -q
225/73:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
225/75: %cpaste -q
226/75:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
226/77: result
226/78: plt.hist(result)
226/79: import matplotlib.pyplot as plt
226/80: plt.hist(result)
226/81: %cpaste -q
227/81:
I = np.array([0,0,0,1])
opt_config = {
    'obj':"attacksize",
    'V_repr':"ratio",
    'constraint_bnd':0.05
}
227/83: %cpaste -q
228/83:
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
228/85: %cpaste -q
229/85:
with multiprocess.Pool(12) as p:
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
229/87: result
229/88: plt.hist(result)
230/1: %cpaste -q
231/1:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
231/3: %cpaste -q
231/4: # set up population dataframe
231/5: # with vaccination rates
231/6:
vacc_df = {
        'pop': [1200,1200,1500,2000],
                'vacc': [0.9,0.9,0.9,0.9]
                }
231/7: vacc_df = pd.DataFrame(vacc_df)
231/8: --
231/9: %cpaste -q
232/9:
"""
Example file demonstrating how to setup and call the oracle
with a very simple example problem instance.
"""
import numpy as np
import pandas as pd
import scripts.optimization.vacc as vacc
import multiprocess
232/11: %cpaste -q
233/11:
# set up population dataframe
# with vaccination rates
vacc_df = {
        'pop': [1200,1200,1500,2000],
        'vacc': [0.9,0.9,0.9,0.9]
}
vacc_df = pd.DataFrame(vacc_df)
233/13: %cpaste -q
234/13:
# set up distance matrix
# all pairwise distances
dist_mat = [
        [0,1,5,5],
        [1,0,5,5],
        [5,5,0,10],
        [5,5,10,0],
]
234/15: %cpaste -q
235/15:
# setup the configuration
# for the disease simulation
tsir_config = {
    "iters":75,    # number of iterations to run sim for
    "tau1":0.7,    # gravity model parameters
    "tau2":1.2,
    "rho": 0.97,
    "theta": 0.05,
    "alpha": 0.97, # mixing rate
    "beta": 7      # disease infectiousness
}
# arguments for optimizer oracle
sim_params = {
        'config':tsir_config,  # contains all disease parameters
        'pop':vacc_df,
        'distances':np.array(dist_mat)
}
235/17: %cpaste -q
236/17:
# optimizer oracle configuration
opt_config = {
    'obj':"attacksize",   # objective function
    'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
    'constraint_bnd':0.05 # set c=0.05 (percentage can go down by 5%)
}
236/19: I = np.array([0,0,0,1])   # seeding: set outbreak to begin in district 4
236/20: %cpaste -q
237/20:
# plug all arguments into oracle
engine = vacc.VaccRateOptEngine(
        opt_config=opt_config,
        V_0=vacc_df['vacc'], seed=I,
        sim_config=tsir_config,
        pop=vacc_df,
        distances=np.array(dist_mat))
237/22: %cpaste -q
238/22:
# setup for multithreading using 5 threads
with multiprocess.Pool(5) as p:
    # query the vector where we uniformly distribute the vaccination decrease over all districts
    result, sim_pool = engine.query(V_delta=0.05*np.ones(4),pool=p,n_sim=150, return_sim_pool=True)
238/24: result
238/25: sim_pool.plot_paths()
238/26: import scripts.optimization.vacc
239/1:
import numpy as np
import matplotlib.pyplot as plt
import scipy
239/2:
# define a constrained set (2d)
simplex_bound = 5
x1_ineq = 6
x2_ineq = 4
239/3:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/4:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/5:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample if sample[0] <=0.5 if sample[1]<= 0.75 if sample[2] <= 0.6]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/6:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample if sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/7:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6) else]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/8:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6) else 0]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/9:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6) for sample in samples]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/10:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
print(np.sum(samples, axis=1))

plt.show()
239/11:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/12: print(samples)
239/13:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constrained = [sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/14:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constr = [sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)]
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/15:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constr = np.array([sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)])
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/16:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constr = np.array([sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)])
fig = plt.figure()
ax = fig.add_subplot(011, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/17:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constr = np.array([sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)])
fig = plt.figure()
ax = fig.add_subplot(11, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/18:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constr = np.array([sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)])
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
239/19: import diversipy.polytope as polytope
239/20: import diversipy
239/21:
diversipy.polytope.sample(
    n_points=100,
    lower=[0,0,0]
    upper=[0.75,0.8,0.9],
    A2=np.array([1,1,1]),
    b=np.array([1])
)
239/22:
diversipy.polytope.sample(
    n_points=100,
    lower=[0,0,0],
    upper=[0.75,0.8,0.9],
    A2=np.array([1,1,1]),
    b2=np.array([1])
)
239/23: from diversipy import polytope
239/24: from diversipy import polytope
239/25:
diversipy.polytope.sample(
    n_points=100,
    lower=[0,0,0],
    upper=[0.75,0.8,0.9],
    A2=np.array([1,1,1]),
    b2=np.array([1])
)
239/26:
diversipy.polytope.sample(
    n_points=100,
    lower=[0,0,0],
    upper=[0.75,0.8,0.9],
    A2=np.array([[1,1,1]]),
    b2=np.array([1])
)
239/27:
samples= diversipy.polytope.sample(
    n_points=100,
    lower=[0,0,0],
    upper=[0.75,0.8,0.9],
    A2=np.array([[1,1,1]]),
    b2=np.array([1])
)
239/28:
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
239/29:
samples= diversipy.polytope.sample(
    n_points=1000,
    lower=[0,0,0],
    upper=[0.75,0.8,0.9],
    A2=np.array([[1,1,1]]),
    b2=np.array([1])
)
239/30:
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
240/1: import hopsy
240/2:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=2)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=[.5, .5])
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/3:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

# next we construct a 2-dim standard Gaussian
model = hopsy.Uniform(dim=2)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=[.5, .5])
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/4: np.random.random()
240/5:
import numpy as np
np.random.random()
240/6:
import numpy as np
np.random.random(10)
240/7: from diversipy import polytope
240/8:
samples= diversipy.polytope.sample(
    n_points=1000,
    lower=np.zeros(3),
    upper=np.random.random(3),
    A2=np.array([[1,1,1]]),
    b2=np.array([1])
)
240/9: from diversipy import polytope
240/10:
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(3),
    upper=np.random.random(3),
    A2=np.array([[1,1,1]]),
    b2=np.array([1])
)
240/11:
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(10),
    upper=np.random.random(10),
    A2=np.array([[1,1,1]]),
    b2=np.array([1])
)
240/12:
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(10),
    upper=np.random.random(10),
    A2=np.array([np.ones(10)]),
    b2=np.array([1])
)
240/13:
dim = 10
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/14:
dim = 100
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/15:
dim = 1000
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/16:
dim = 700
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/17:
dim = 700
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/18:
dim = 700
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/19:
dim = 705
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/20:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=2)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=[.5, .5])
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/21:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=[.5, .5])
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/22:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=[0, 0])
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/23:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/24:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10_000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/25:
dim = 705
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/26:
dim = 705
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/27:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/28:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/29:
dim = 705
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/30:
class Uniform_Model:
    def __init__(self, dims,lb_vec,ub_vec):
        self.volume =  np.prod(ub_vec - lb_vec)
        self.nll = -np.log(1/self.volume)
        self.ub_vec = ub_vec
        self.lb_vec = lb_vec
    def compute_negative_log_likelihood(self, x):
        if np.all(self.lb_vec <= x) and np.all(x <= self.ub_vec):
            return self.nll
        else:
            return 0
240/31: 5 == 5 == 5
240/32:
class Uniform_Model:
    def __init__(self, dims,lb_vec,ub_vec):
        self.volume =  np.prod(ub_vec - lb_vec)
        self.nll = -np.log(1/self.volume)
        self.ub_vec = ub_vec
        self.lb_vec = lb_vec
        assert len(lb_vec) == len(ub_vec) == dims
    def compute_negative_log_likelihood(self, x):
        if np.all(self.lb_vec <= x) and np.all(x <= self.ub_vec):
            return self.nll
        else:
            return 0
240/33: Uniform_Model(5, np.array([0,0,0,0]), np.array([1,1,1,1]))
240/34:
class Uniform_Model:
    def __init__(self, dims,lb_vec,ub_vec):
        self.volume =  np.prod(ub_vec - lb_vec)
        self.nll = -np.log(1/self.volume)
        self.ub_vec = np.array(ub_vec)
        self.lb_vec = np.array(lb_vec)
        assert len(lb_vec) == len(ub_vec) == dims
    def compute_negative_log_likelihood(self, x):
        if np.all(self.lb_vec <= x) and np.all(x <= self.ub_vec):
            return self.nll
        else:
            return 0
240/35: Uniform_Model(5, np.array([0,0,0,0]), np.array([1,1,1,1]))
240/36: Uniform_Model(4, np.array([0,0,0,0]), np.array([1,1,1,1]))
240/37: Uniform_Model(4, np.array([0,0,0,0]), np.array([1,1,1,1])).compute_negative_log_likelihood([0.5,0.5,0.5,0.5])
240/38: Uniform_Model(4, np.array([0,0,0,0]), np.array([1,1,1,1])).nll
240/39:
class Uniform_Model:
    def __init__(self, dims,lb_vec,ub_vec):
        self.volume =  np.prod(ub_vec - lb_vec)
        print(self.volume)
        self.nll = -np.log(1/self.volume)
        self.ub_vec = np.array(ub_vec)
        self.lb_vec = np.array(lb_vec)
        assert len(lb_vec) == len(ub_vec) == dims
    def compute_negative_log_likelihood(self, x):
        if np.all(self.lb_vec <= x) and np.all(x <= self.ub_vec):
            return self.nll
        else:
            return 0
240/40: Uniform_Model(4, np.array([0,0,0,0]), np.array([1,1,1,1])).nll
240/41: np.log(0)
240/42:
class Uniform_Model:
    def __init__(self, dims,lb_vec,ub_vec):
        self.volume =  np.prod(ub_vec - lb_vec)
        print(self.volume)
        self.nll = -np.log(1/self.volume)
        self.ub_vec = np.array(ub_vec)
        self.lb_vec = np.array(lb_vec)
        assert len(lb_vec) == len(ub_vec) == dims
    def compute_negative_log_likelihood(self, x):
        if np.all(self.lb_vec <= x) and np.all(x <= self.ub_vec):
            return self.nll
        else:
            return np.inf
240/43: Uniform_Model(4, np.array([0,0,0,0]), np.array([1,1,1,1])).nll
240/44: Uniform_Model(4, np.array([0,0,0,0]), np.array([-1,1,1,1])).nll
240/45: Uniform_Model(4, np.array([0,0,0,0]), np.array([1,1,1,1])).nll
240/46:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/47:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=[0,0],ub_vec=[1,1])

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/48:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/49:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/50:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/51:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint())

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/52:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(1))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/53:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/54:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/55:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/56:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/57:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/58:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/59:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/60:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/61:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/62:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/63:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=100000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/64:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/65:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/66:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
hopsy.add_box_constraints(lower_bound=np.zeros(2),upper_bound=0.5*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/67:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.5*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/68:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.75*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/69:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.35*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/70:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.45*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/71:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/72:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/73:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/74:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/75:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/76:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/77:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=10000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/78:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/79:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/80:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=0.9*np.ones(2))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/81:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.4,0.8]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/82:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.4,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/83:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/84:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/85:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/86:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/87:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/88:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/89:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/90:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/91:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/92:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/93:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=10)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/94:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=100000, thinning=2,n_threads=10)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/95:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=100000, thinning=2,n_threads=7)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/96:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=100000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/97:
dim = 705
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/98:
dim = 705
samples= polytope.sample(
    n_points=1000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/99:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/100:
dim = 705
samples= polytope.sample(
    n_points=10000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/101:
dim = 705
samples= polytope.sample(
    n_points=50000,
    lower=np.zeros(dim),
    upper=np.random.random(dim),
    A2=np.array([np.ones(dim)]),
    b2=np.array([1])
)
240/102:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/103:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/104:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [[1, 0], [0, 1], [-1, 0], [0, -1]]
b = [5, 5, 0, 0]

A = [np.ones(dim)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = hopsy.Gaussian(dim=dim)

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)

# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.GaussianHitAndRunProposal, starting_point=np.zeros(dim))
rng = hopsy.RandomNumberGenerator(seed=42)

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=50000, thinning=2, n_threads=7)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/105:
class Uniform_Model:
    def __init__(self, dims,lb_vec,ub_vec):
        self.volume =  np.prod(ub_vec - lb_vec)
        print(self.volume)
        self.nll = -np.log(1/self.volume)
        self.ub_vec = np.array(ub_vec)
        self.lb_vec = np.array(lb_vec)
        assert len(lb_vec) == len(ub_vec) == dims
    def compute_negative_log_likelihood(self, x):
        if np.all(self.lb_vec <= x) and np.all(x <= self.ub_vec):
            return self.nll
        else:
            return np.inf
        
class Unitline:
    def __init__(self):
        pass
    def compute_negative_log_likelihood(self, x):
        if np.isclose(np.sum(x),1) and all(np.array([0,0]) <= x) and all(x<=np.array([1,1])):
            return -np.log(1/(0.5))
        else:
            return np.inf
240/106:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Unitline()

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
#problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/107:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Unitline()

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([1,1]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/108:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Unitline()

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([1,1]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/109:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(2)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=2,lb_vec=np.array([0,0]),ub_vec=np.array([1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(2),upper_bound=np.array([0.6,0.9]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/110:
# analytical map
import numpy as np
import matplotlib.pyplot as plt

def sample_simplex(xi, n_mc=1, on_simplex=False):
    """
    Use an analytical function map to points in the d-dimensional unit hypercube to a
    d-dimensional simplex with nodes xi.

    Parameters
    ----------
    xi: array of floats, shape (d + 1, d)
        The nodes of the d dimensional simplex.

    n_mc : int, The default is 1.
        Number of samples to draw from inside the simplex

    on_simplex: boolean, default is False
        If True, sample on the simplex rather than inside it

    Returns: array, shape (n_mc, d)
    -------
    n_mc uniformly distributed points inside the d-dimensional simplex with edges xi.

    """
    d = xi.shape[1]
    P = np.zeros([n_mc, d])
    for k in range(n_mc):
        # random points inside the unit hypercube
        r = np.random.rand(d)
        # sample on, instead of inside the simplex (all samples will sum to one)
        if on_simplex:
            r[-1] = 1

        # the term of the map is \xi_k_j0
        sample = np.copy(xi[0])
        for i in range(1, d + 1):
            prod_r = 1.
            # compute the product of r-terms: prod(r_{d-j+1}^{1/(d-j+1)})
            for j in range(1, i + 1):
                prod_r *= r[d - j]**(1. / (d - j + 1))
            # compute the ith term of the sum: prod_r*(\xi_i-\xi_{i-1})
            sample += prod_r * (xi[i] - xi[i - 1])
        P[k, :] = sample

    return P

plt.close('all')

# triangle points
xi = np.array([[0.0, 0.0], [1.0, 0.0], [0, 1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
fig = plt.figure()
ax = fig.add_subplot(111)
# plot random samples
ax.plot(samples[:,0], samples[:, 1], '.')
# plot edges
ax.plot(xi[:, 0], xi[:, 1], 'o')
plt.tight_layout()

# 3D simplex
xi = np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0, 1, 0], [0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
samples_constr = np.array([sample for sample in samples if (sample[0] <=0.5 and sample[1]<= 0.75 and sample[2] <= 0.6)])
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# plot random samples
#ax.scatter(samples[:,0], samples[:, 1], samples[:,2], '.')
ax.scatter(samples_constr[:,0], samples_constr[:, 1], samples_constr[:,2], '.')
# plot edges
ax.scatter(xi[:, 0], xi[:, 1], xi[:, 2], 'o')
plt.tight_layout()

# 4D simplex
xi = np.array([[0,0,0,0], [1.0, 0.0, 0.0, 0], [0, 1, 0, 0], [0,0,1,0], [0,0,0,1]])
samples = sample_simplex(xi, 1000, on_simplex=True)
# These should all sum to one
#print(np.sum(samples, axis=1))

plt.show()
240/111:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(3)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=3,lb_vec=np.array([0,0,0]),ub_vec=np.array([1,1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(3),upper_bound=np.array([0.6,0.9,0.6]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(2))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
240/112:
import hopsy
import matplotlib.pyplot as plt

# the polytope is defined as 
#          P := {x : Ax <= b}
# thus we need to define A and b. these constraints form the simple box [0,5]^2.
A = [np.ones(3)]
b = np.array([1])
# next we construct a 2-dim standard Gaussian
model = Uniform_Model(dims=3,lb_vec=np.array([0,0,0]),ub_vec=np.array([1,1,1]))

# the complete problem is defined by the target distribution and the constrained domain, 
# defined by the above mentioned inequality
problem = hopsy.Problem(A, b, model)
problem=hopsy.add_box_constraints(problem=problem,lower_bound=np.zeros(3),upper_bound=np.array([0.6,0.9,0.6]))
# the run object contains and constructs the markov chains. in the default case, the
# Run object will have a single chain using the Hit-and-Run proposal algorithm and is
# set to produce 10,000 samples.
mc = hopsy.MarkovChain(problem, proposal=hopsy.UniformCoordinateHitAndRunProposal, starting_point=np.zeros(3))
rng = hopsy.RandomNumberGenerator(seed=np.random.randint(100))

# call sample on the mc and rng objects 
acceptance_rate, states = hopsy.sample(mc, rng, n_samples=1000, thinning=2,n_threads=1)

# the states have 3 dimensions: number of chains, number of samples, number of dimensions.
plt.scatter(states[:,:,0].flatten(), states[:,:,1].flatten())
plt.show()
241/1: import functions
241/2: functions
241/3: functions.functions.Ackley
241/4: functions.Ackley
241/5: from functions.functions import *
241/6: from functions.functions import *
241/7: f = Ackley(dims=2)
241/8: f([0.1,0.1])
241/9: f(np.array([0.1,0.1]))
241/10: x = np.arange(-10,10)
241/11: x
241/12:
x = np.arange(-10,10)
y = np.arange(-10,10)

xy_pairs = [ np.array([x_j,y_j]) for x_j in x for y_j in y]
241/13: f(xy_pairs)
241/14: [f(xy) for xy in xy_pairs]
241/15: xy_pairs[0]
241/16: f(xy_pairs[0])
241/17:
x = np.arange(-5,10)
y = np.arange(-5,10)

xy_pairs = [ np.array([x_j,y_j]) for x_j in x for y_j in y]
241/18: f(xy_pairs[0])
241/19: [f(xy) for xy in xy_pairs]
241/20: f_xy=[f(xy) for xy in xy_pairs]
241/21: f(np.array([0.1]))
241/22: f = Ackley(dims=1)
241/23: f(np.array([0.1]))
241/24: f_xy=[f(x) for x in np.arange(-5,10)]
241/25: f_x[f(np.array([x])) for x in np.arange(-5,10)]
241/26: f_x[f(np.array([x]) for x in np.arange(-5,10)]
241/27: f_x = [f(np.array([x]) for x in np.arange(-5,10)]
241/28: f_x = [ f(np.array([x])) for x in np.arange(-5,10)]
241/29: f_x
241/30: plt.plot(np.arange(-5,10),f_x)
241/31:
from functions.functions import *
import matplotlib.pyplot as plt
241/32: plt.plot(np.arange(-5,10),f_x)
241/33: f_x = [ f(np.array([x])) for x in np.arange(-5,10)]
241/34: plt.plot(np.arange(-5,10),f_x)
241/35: f = Ackley(dims=2)
241/36: f(np.array([0.1,0.1]))
241/37:
x = np.arange(-5,10)
y = np.arange(-5,10)

xy_pairs = [ np.array([x_j,y_j]) for x_j in x for y_j in y]
241/38:
#f_x = [ f(np.array([x])) for x in np.arange(-5,10)]
f_xy = [f(xy) for xy in xy_pairs]
241/39:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt
 
# defining surface and axes
x = np.outer(np.linspace(-2, 2, 10), np.ones(10))
y = x.copy().T
z = np.cos(x ** 2 + y ** 3)
 
fig = plt.figure()
 
# syntax for 3-D plotting
ax = plt.axes(projection ='3d')
 
# syntax for plotting
ax.plot_surface(x, y, z, cmap ='viridis', edgecolor ='green')
ax.set_title('Surface plot geeks for geeks')
plt.show()
241/40: x
241/41:
x = np.arange(-5,10)
y = np.arange(-5,10)

xy_pairs = [ np.array([x_j,y_j]) for x_j in x for y_j in y]
241/42:
#f_x = [ f(np.array([x])) for x in np.arange(-5,10)]
f_xy = [f(xy) for xy in xy_pairs]
241/43:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt

fig = plt.figure()
 
# syntax for 3-D plotting
ax = plt.axes(projection ='3d')
 
# syntax for plotting
ax.plot_surface(x, y, f_xy, cmap ='viridis', edgecolor ='green')
ax.set_title('Surface plot geeks for geeks')
plt.show()
241/44:
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt

fig = plt.figure()
 
# syntax for 3-D plotting
ax = plt.axes(projection ='3d')
 
# syntax for plotting
ax.plot_surface(x, y, f_xy, cmap ='viridis', edgecolor ='green')
ax.set_title('Surface plot geeks for geeks')
plt.show()
241/45: x
241/46: y
241/47: xy_pairs
241/48:
np.array([[ 9.97014908e+00 -1.11525895e+00]
 [ 9.18999637e+00 -3.93095610e+00]
 [-4.39092441e+00 -6.33279938e-01]
 [-2.37820086e+00 -4.91818166e+00]
 [ 2.36077385e+00  2.47774421e+00]
 [ 3.01624587e+00  1.09639716e+00]
 [ 4.57271153e+00 -1.32731520e+00]
 [ 2.02356009e+00  6.55271986e+00]
 [ 1.56705500e+00  6.21712481e+00]
 [-2.32811291e+00  5.16257597e+00]
 [-6.57925178e-01  6.00616203e+00]
 [-1.57373780e+00  6.04377497e+00]
 [ 9.55844634e-01 -1.01092434e+00]
 [ 2.71082750e-01 -3.85070206e+00]
 [ 2.78111124e-01 -4.43629905e+00]
 [ 8.65402150e-01 -3.70043280e+00]
 [ 2.39704861e+00 -1.81720849e+00]
 [ 3.16336924e+00  1.16684899e+00]
 [ 3.99929561e+00  2.23733844e+00]
 [ 2.51018595e+00  3.17419811e+00]
 [-3.86877012e-01 -3.32366940e+00]
 [-9.30062484e-01 -4.38875131e+00]
 [ 1.62632170e+00 -4.00288528e+00]
 [ 1.64218001e+00 -3.98438026e+00]
 [-1.24084915e+00 -2.75556876e+00]
 [ 7.02648783e-02 -4.22142752e+00]
 [ 4.78162201e+00  4.95006408e+00]
 [ 1.24629469e+00 -1.81510708e-01]
 [-7.17885404e-01 -7.96509404e-01]
 [-3.83820763e+00  1.83872170e+00]
 [-8.88203492e-01  4.21112279e+00]
 [ 1.39092626e+00  8.29705570e+00]
 [ 2.13258631e+00  6.79539071e+00]
 [ 5.84623035e+00 -1.11687526e+00]
 [ 1.57255766e-01  9.31476730e+00]
 [ 4.00678103e-01  9.46753191e+00]
 [-9.97853061e-01  8.83793971e+00]
 [-2.52723202e+00  7.18246655e+00]
 [ 1.01250399e+00  7.99408417e+00]
 [ 7.52716087e-01  7.85301150e+00]
 [ 2.47116684e-01 -1.03547344e+00]
 [-1.40623131e+00  1.05335637e-02]
 [-3.46473889e-01  1.17868413e+00]
 [-1.19217120e-01  8.34674742e-03]
 [ 9.53511966e-02  1.90124526e-01]
 [-3.55173889e-01  8.92624755e-02]
 [ 1.38447289e-01 -2.85513548e-01]
 [-2.18606931e+00 -7.42358402e-01]
 [-7.29962516e-02 -3.41829704e-02]])
241/49:
np.array([[ 9.97014908e+00 -1.11525895e+00],
 [ 9.18999637e+00 -3.93095610e+00],
 [-4.39092441e+00 -6.33279938e-01],
 [-2.37820086e+00 -4.91818166e+00],
 [ 2.36077385e+00  2.47774421e+00],
 [ 3.01624587e+00  1.09639716e+00],
 [ 4.57271153e+00 -1.32731520e+00],
 [ 2.02356009e+00  6.55271986e+00],
 [ 1.56705500e+00  6.21712481e+00],
 [-2.32811291e+00  5.16257597e+00],
 [-6.57925178e-01  6.00616203e+00],
 [-1.57373780e+00  6.04377497e+00],
 [ 9.55844634e-01 -1.01092434e+00],
 [ 2.71082750e-01 -3.85070206e+00],
 [ 2.78111124e-01 -4.43629905e+00],
 [ 8.65402150e-01 -3.70043280e+00],
 [ 2.39704861e+00 -1.81720849e+00],
 [ 3.16336924e+00  1.16684899e+00],
 [ 3.99929561e+00  2.23733844e+00],
 [ 2.51018595e+00  3.17419811e+00],
 [-3.86877012e-01 -3.32366940e+00],
 [-9.30062484e-01 -4.38875131e+00],
 [ 1.62632170e+00 -4.00288528e+00],
 [ 1.64218001e+00 -3.98438026e+00],
 [-1.24084915e+00 -2.75556876e+00],
 [ 7.02648783e-02 -4.22142752e+00],
 [ 4.78162201e+00  4.95006408e+00],
 [ 1.24629469e+00 -1.81510708e-01],
 [-7.17885404e-01 -7.96509404e-01],
 [-3.83820763e+00  1.83872170e+00],
 [-8.88203492e-01  4.21112279e+00],
 [ 1.39092626e+00  8.29705570e+00],
 [ 2.13258631e+00  6.79539071e+00],
 [ 5.84623035e+00 -1.11687526e+00],
 [ 1.57255766e-01  9.31476730e+00],
 [ 4.00678103e-01  9.46753191e+00],
 [-9.97853061e-01  8.83793971e+00],
 [-2.52723202e+00  7.18246655e+00],
 [ 1.01250399e+00  7.99408417e+00],
 [ 7.52716087e-01  7.85301150e+00],
 [ 2.47116684e-01 -1.03547344e+00],
 [-1.40623131e+00  1.05335637e-02],
 [-3.46473889e-01  1.17868413e+00],
 [-1.19217120e-01  8.34674742e-03],
 [ 9.53511966e-02  1.90124526e-01],
 [-3.55173889e-01  8.92624755e-02],
 [ 1.38447289e-01 -2.85513548e-01],
 [-2.18606931e+00 -7.42358402e-01],
 [-7.29962516e-02 -3.41829704e-02]])
241/50:
np.array(
[[ 9.97014908e+00, -1.11525895e+00],
 [ 9.18999637e+00, -3.93095610e+00],
 [-4.39092441e+00, -6.33279938e-01],
 [-2.37820086e+00, -4.91818166e+00],
 [ 2.36077385e+00,  2.47774421e+00],
 [ 3.01624587e+00,  1.09639716e+00],
 [ 4.57271153e+00, -1.32731520e+00],
 [ 2.02356009e+00,  6.55271986e+00],
 [ 1.56705500e+00,  6.21712481e+00],
 [-2.32811291e+00,  5.16257597e+00],
 [-6.57925178e-01,  6.00616203e+00],
 [-1.57373780e+00,  6.04377497e+00],
 [ 9.55844634e-01, -1.01092434e+00],
 [ 2.71082750e-01, -3.85070206e+00],
 [ 2.78111124e-01, -4.43629905e+00],
 [ 8.65402150e-01, -3.70043280e+00],
 [ 2.39704861e+00, -1.81720849e+00],
 [ 3.16336924e+00,  1.16684899e+00],
 [ 3.99929561e+00,  2.23733844e+00],
 [ 2.51018595e+00,  3.17419811e+00],
 [-3.86877012e-01, -3.32366940e+00],
 [-9.30062484e-01, -4.38875131e+00],
 [ 1.62632170e+00, -4.00288528e+00],
 [ 1.64218001e+00, -3.98438026e+00],
 [-1.24084915e+00, -2.75556876e+00],
 [ 7.02648783e-02, -4.22142752e+00],
 [ 4.78162201e+00,  4.95006408e+00],
 [ 1.24629469e+00, -1.81510708e-01],
 [-7.17885404e-01, -7.96509404e-01],
 [-3.83820763e+00,  1.83872170e+00],
 [-8.88203492e-01,  4.21112279e+00],
 [ 1.39092626e+00,  8.29705570e+00],
 [ 2.13258631e+00,  6.79539071e+00],
 [ 5.84623035e+00, -1.11687526e+00],
 [ 1.57255766e-01,  9.31476730e+00],
 [ 4.00678103e-01,  9.46753191e+00],
 [-9.97853061e-01,  8.83793971e+00],
 [-2.52723202e+00,  7.18246655e+00],
 [ 1.01250399e+00,  7.99408417e+00],
 [ 7.52716087e-01,  7.85301150e+00],
 [ 2.47116684e-01, -1.03547344e+00],
 [-1.40623131e+00,  1.05335637e-02],
 [-3.46473889e-01,  1.17868413e+00],
 [-1.19217120e-01,  8.34674742e-03],
 [ 9.53511966e-02,  1.90124526e-01],
 [-3.55173889e-01,  8.92624755e-02],
 [ 1.38447289e-01, -2.85513548e-01],
 [-2.18606931e+00, -7.42358402e-01],
 [-7.29962516e-02, -3.41829704e-02]])
241/51:
x =np.array(
[[ 9.97014908e+00, -1.11525895e+00],
 [ 9.18999637e+00, -3.93095610e+00],
 [-4.39092441e+00, -6.33279938e-01],
 [-2.37820086e+00, -4.91818166e+00],
 [ 2.36077385e+00,  2.47774421e+00],
 [ 3.01624587e+00,  1.09639716e+00],
 [ 4.57271153e+00, -1.32731520e+00],
 [ 2.02356009e+00,  6.55271986e+00],
 [ 1.56705500e+00,  6.21712481e+00],
 [-2.32811291e+00,  5.16257597e+00],
 [-6.57925178e-01,  6.00616203e+00],
 [-1.57373780e+00,  6.04377497e+00],
 [ 9.55844634e-01, -1.01092434e+00],
 [ 2.71082750e-01, -3.85070206e+00],
 [ 2.78111124e-01, -4.43629905e+00],
 [ 8.65402150e-01, -3.70043280e+00],
 [ 2.39704861e+00, -1.81720849e+00],
 [ 3.16336924e+00,  1.16684899e+00],
 [ 3.99929561e+00,  2.23733844e+00],
 [ 2.51018595e+00,  3.17419811e+00],
 [-3.86877012e-01, -3.32366940e+00],
 [-9.30062484e-01, -4.38875131e+00],
 [ 1.62632170e+00, -4.00288528e+00],
 [ 1.64218001e+00, -3.98438026e+00],
 [-1.24084915e+00, -2.75556876e+00],
 [ 7.02648783e-02, -4.22142752e+00],
 [ 4.78162201e+00,  4.95006408e+00],
 [ 1.24629469e+00, -1.81510708e-01],
 [-7.17885404e-01, -7.96509404e-01],
 [-3.83820763e+00,  1.83872170e+00],
 [-8.88203492e-01,  4.21112279e+00],
 [ 1.39092626e+00,  8.29705570e+00],
 [ 2.13258631e+00,  6.79539071e+00],
 [ 5.84623035e+00, -1.11687526e+00],
 [ 1.57255766e-01,  9.31476730e+00],
 [ 4.00678103e-01,  9.46753191e+00],
 [-9.97853061e-01,  8.83793971e+00],
 [-2.52723202e+00,  7.18246655e+00],
 [ 1.01250399e+00,  7.99408417e+00],
 [ 7.52716087e-01,  7.85301150e+00],
 [ 2.47116684e-01, -1.03547344e+00],
 [-1.40623131e+00,  1.05335637e-02],
 [-3.46473889e-01,  1.17868413e+00],
 [-1.19217120e-01,  8.34674742e-03],
 [ 9.53511966e-02,  1.90124526e-01],
 [-3.55173889e-01,  8.92624755e-02],
 [ 1.38447289e-01, -2.85513548e-01],
 [-2.18606931e+00, -7.42358402e-01],
 [-7.29962516e-02, -3.41829704e-02]])
241/52: plt.scatter(x)
241/53: plt.scatter(x=[k[0] for k in x], y= [k[1] for k in x])
241/54: np.sum(x)
241/55: np.sum(x,axis=0)
241/56: np.sum(x,axis=1)
242/1: import datetime
242/2: datetime.datetime.now()
242/3: print(datetime.datetime.now())
242/4: print(datetime.datetime.now.isoformat())
242/5: print(datetime.datetime.now().isoformat())
242/6: datetime.datetime.now().isoformat()
243/1: import vacc
244/1: import scripts.optimization.vacc
245/1: import scripts.optimization.vacc
246/1: import scripts.optimization.vacc
246/2: import pandas as pd
246/3:
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)
246/4:
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }
246/5: import numpy as np
246/6:
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }
246/7:
    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05 # set c=0.05 (percentage can go down by 5%)
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
246/8: v = VaccProblemLAMCTSWrapper(opt_config, V_0, I, sim_config, pop, distances, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/9: v = vacc.VaccProblemLAMCTSWrapper(opt_config, V_0, I, sim_config, pop, distances, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/10: vacc
246/11: vacc
246/12: import scripts.optimization.vacc as vacc
246/13: v = vacc.VaccProblemLAMCTSWrapper(opt_config, V_0, I, sim_config, pop, distances, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/14: V_0
246/15: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_config, pop, distances, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/16: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, pop, distances, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/17: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df['pop'], distances, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/18: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df['pop'], dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/19:
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }
246/20: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df['pop'], dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/21: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df, dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
246/22: import multiprocess
246/23: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df, dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
247/1: import scripts.optimization.vacc as vacc
247/2: import pandas as pd
247/3: import numpy as np
247/4:
    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
247/5: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df['pop'], dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
247/6: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df, dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
247/7: v(np.array([0,0.1,0.1,0,0]))
248/1:
    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
248/2: import pandas as pd; import numpy as np
248/3:
    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
248/4: import scripts.optimization.vacc as vacc
248/5:
    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
248/6: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df, dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
248/7: v(np.array([0,0.1,0.1,0,0]))
249/1: import scripts.optimization.vacc as vacc
249/2:
    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
249/3: import pandas as pd; import numpy as np
249/4:
    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4
249/5: v = vacc.VaccProblemLAMCTSWrapper(opt_config, vacc_df['vacc'], I, sim_params, vacc_df, dist_mat, 7, 100, "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/")
249/6: v(np.array([0,0.1,0.1,0,0]))
250/1: import pandas as pd; import numpy as np
250/2: import scripts.optimization.vacc as vacc
250/3:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = sim_config, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
250/4:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = sim_params, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
250/5: v(0.05*np.ones(5))
250/6:
    # plug all arguments into oracle
    engine = vacc.VaccRateOptEngine(
            opt_config=opt_config,
            V_0=vacc_df['vacc'], seed=I,
            sim_config=tsir_config,
            pop=vacc_df,
            distances=np.array(dist_mat))

    # setup for multithreading using 5 threads
    with multiprocess.Pool(10) as p:
        # query the vector where we uniformly distribute the vaccination decrease over all districts
        result, sim_pool = engine.query(V_delta=0.05*np.ones(5),pool=p,n_sim=150, return_sim_pool=True)
        vacc_bayes_opt_w_constr(
                n_initial_pts=20,
                vacc_engine=engine,
                n_sim=150,
                pool=p,
                max_iters=10
                )
        print(np.mean(result))
250/7: import multiprocess
250/8:
    # plug all arguments into oracle
    engine = vacc.VaccRateOptEngine(
            opt_config=opt_config,
            V_0=vacc_df['vacc'], seed=I,
            sim_config=tsir_config,
            pop=vacc_df,
            distances=np.array(dist_mat))

    # setup for multithreading using 5 threads
    with multiprocess.Pool(10) as p:
        # query the vector where we uniformly distribute the vaccination decrease over all districts
        result, sim_pool = engine.query(V_delta=0.05*np.ones(5),pool=p,n_sim=150, return_sim_pool=True)
        vacc_bayes_opt_w_constr(
                n_initial_pts=20,
                vacc_engine=engine,
                n_sim=150,
                pool=p,
                max_iters=10
                )
        print(np.mean(result))
250/9:
    # plug all arguments into oracle
    engine = vacc.VaccRateOptEngine(
            opt_config=opt_config,
            V_0=vacc_df['vacc'], seed=I,
            sim_config=tsir_config,
            pop=vacc_df,
            distances=np.array(dist_mat))
250/10:
with multiprocess.Pool(5) as p:
            # query the vector where we uniformly distribute the vaccination decrease over all districts
        result, sim_pool = engine.query(V_delta=0.05*np.ones(5),pool=p,n_sim=150, return_sim_pool=True)
250/11: result
250/12: sim_pool.plot_paths()
250/13: %matplotlib qt
250/14: sim_pool.plot_paths()
250/15: result
250/16: v
250/17: v(0.01*np.ones(5))
250/18: v.pool
250/19: v.n_sim
250/20: v.engine
250/21: v.engine.query(V_delta=0.05*np.ones(5),pool=v.pool, n_sim=v.n_sim)
251/1: import scripts.optimization.vacc as vacc
251/2: import numpy as np; import pandas as pd
251/3:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = sim_params, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
251/4: v(0.01*np.ones(5))
251/5: v.engine.query(V_delta=0.05*np.ones(5),pool=v.pool, n_sim=v.n_sim)
251/6: v.engine.query(V_delta=0.05*np.ones(5),pool=v.pool, n_sim=v.n_sim)
251/7: v(0.01*np.ones(5))
251/8:
    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = tsir_config, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
251/9: v(0.01*np.ones(5))
251/10: v(0.01*np.ones(5))
252/1:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = sim_params, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
252/2: import numpy as np; import pandas as pd
252/3:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = sim_params, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
252/4: import scripts.optimization.vacc as vacc
252/5:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = sim_params, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
252/6: v(0.01*np.ones(5))
252/7:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = tsir_config, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
252/8: v(0.01*np.ones(5))
252/9: v(0.01*np.ones(5))
252/10: v(0.01*np.ones(5))
252/11: v(0.01*np.ones(5))
252/12: v(0.01*np.ones(5))
252/13: v(0.01*np.ones(5))
252/14: v(0.02*np.ones(5))
252/15: v(0.02*np.ones(5))
252/16: v(0.02*np.ones(5))
252/17: v(0.02*np.ones(5))
252/18: v(0.03*np.ones(5))
252/19: v(0.04*np.ones(5))
252/20: v(0.05*np.ones(5))
252/21: v(0.05*np.ones(5))
252/22: v(0.05*np.ones(5))
252/23: v(0.05*np.ones(5))
252/24: v([0,0,0.05,0,0])
252/25: v([0,0,0.15,0,0])
252/26: v([0,0,0.15,0,0])
252/27: v([0,0,0.15,0,0])
252/28: [0.1,0.1,0.1]
252/29: np.array([0.1,0.1,0.1])
252/30: "".join(np.array([0.1,0.1,0.1]))
252/31: np.savetxt(np.array([0.1,0.1,0.1]))
252/32: np.savetxt(np.array([0.1,0.1,0.1]))
253/1: import scripts.optimization.vacc as vacc
253/2: import numpy as np; import pandas as pd
253/3:
    # NOTE: we assume the labeling is consistent
    # between vacc_df and dist_mat

    # set up population dataframe
    # with vaccination rates
    vacc_df = {
            'pop': [1200, 1200, 4000, 2000, 5000],
            'vacc': [0.9,0.9,0.9,0.9, 0.75]
    }
    vacc_df = pd.DataFrame(vacc_df)

    # set up distance matrix
    # all pairwise distances
    dist_mat = [
            [0,1,5,5,1],
            [1,0,5,5,1],
            [5,5,0,10,1],
            [5,5,10,0,1],
            [5,5,10,1,0]
    ]

    # setup the configuration
    # for the disease simulation
    tsir_config = {
        "iters": 75,    # number of iterations to run sim for
        "tau1": 0.7,    # gravity model parameters
        "tau2": 1.2,
        "rho": 0.97,
        "theta": 0.05,
        "alpha": 0.97, # mixing rate
        "beta": 7      # disease infectiousness
    }
    # arguments for optimizer oracle
    sim_params = {
            'config':tsir_config,  # contains all disease parameters
            'pop':vacc_df,
            'distances':np.array(dist_mat)
    }

    # optimizer oracle configuration
    opt_config = {
        'obj':"attacksize",   # objective function
        'V_repr':"ratio",     # represent vacc rates as a ratio: [0,1]
        'constraint_bnd':0.05, # set c=0.05 (percentage can go down by 5%)
        'constraint_type':'ineq'
    }

    I = np.array([0,0,1,0,0])   # seeding: set outbreak to begin in district 4

    v = vacc.VaccProblemLAMCTSWrapper(
            opt_config = opt_config, 
            V_0=vacc_df['vacc'], 
            seed = I,
            sim_config = tsir_config, 
            pop = vacc_df, 
            distances = np.array(dist_mat),
            cores=5, n_sim=10,
            output_dir = "/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/output/"
        )
253/4: v([0,0,0.15,0,0])
253/5: v([0,0,0.15,0,0])
253/6: v([0,0,0.15,0,0])
253/7: v([0,0,0.15,0,0])
253/8: v([0,0,0.15,0,0])
254/1: import pandas as pd; import numpy as np
254/2: x = np.arange(0,100)
254/3: x
254/4: x = np.arange(0,100,10)
254/5: x
254/6: y=x
254/7: x
254/8: y
254/9: points = [(x_i,y_i) for x_i in x for y_i in y]
254/10: points
254/11: pd.DataFrame(points)
254/12: points = pd.DataFrame(points)
254/13: points
254/14: x = np.arange(0,50,10)
254/15: y=x
254/16: points = [(x_i,y_i) for x_i in x for y_i in y]
254/17: points = pd.DataFrame(points)
254/18: points
254/19:
points[1
]
254/20: points[1,:]
254/21: points.iloc[1,:]
254/22: points.iloc[0,:]
254/23: points
254/24: []
254/25:
for i in range(25):
    for j in range(25):
        p1 = points.iloc[i,:]
        p2 = points.iloc[j,:]
        dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**(1/2)
254/26:
for i in range(25):
    for j in range(25):
        p1 = points.iloc[i,:]
        p2 = points.iloc[j,:]
        dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**(1/2)
        print(dist)
254/27:
points_dist = []
for i in range(25):
    for j in range(25):
        p1 = points.iloc[i,:]
        p2 = points.iloc[j,:]
        dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**(1/2)
        points_dist.append((p1,p2,dist))
254/28: points_dist
254/29:
points_dist = []
for i in range(25):
    for j in range(25):
        p1 = points.iloc[i,:]
        p2 = points.iloc[j,:]
        dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**(1/2)
        points_dist.append((p1[0],p1[1],p2[0],p2[1],dist))
254/30: points_dist
254/31: pd.DataFrame(points_dist)
254/32:
points_dist = []
for i in range(25):
    for j in range(25):
        p1 = points.iloc[i,:]
        p2 = points.iloc[j,:]
        dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**(1/2)
        points_dist.append((i,j,dist))
254/33: pd.DataFrame(points_dist)
254/34: pd.DataFrame(points_dist).to_csv()
254/35: pd.DataFrame(points_dist).to_csv("/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/config/5_by_5_dist.csv")
254/36: points
254/37: points.columns = ['x','y']
254/38: points
254/39: points.to_csv("/home/nicholasw/Documents/sync/UVA files/Semesters/Semester 7/CS 4501/project/CS_4501_Fall22_RL_Project/config/5_by_5_pop.csv")
255/1: import configparser
255/2: configparser.ConfigParser
255/3: p = configparser.ConfigParser()
255/4: p.read('params.config')
255/5: p = p.read('params.config')
255/6:
with open('params.config') as p_file:
    p.read(p_file)
255/7: p_file
255/8:
with open('params.config') as p_file:
    p.read(p_file.read())
255/9: p
255/10: p = configparser.ConfigParser()
255/11:
with open('params.config') as p_file:
    p.read(p_file)
255/12: p
255/13: p['tsir_config']
255/14: p
255/15: p.sections()
255/16: p
255/17: p = configparser.ConfigParser()
255/18: p.read(r'params.config')
255/19: p
255/20: p.read(open(r'params.config'))
255/21: ls()
255/22: ls
255/23: open('params.cfg')
255/24: open('params.cfg').read()
255/25: p.readfp(open('params.cfg').read())
255/26: p.readfp(open('params.cfg'))
255/27: p
255/28: p.sections()
255/29: p.read_file(open('params.cfg'))
255/30: p.sections()
255/31: p['tsir_config']
255/32: p['tsir_config']
255/33: dict(p['tsir_config'])
255/34: p.read_file(open('params.cfg'))
255/35: p.sections()
255/36: p['tsir_config']
255/37: print(p['tsir_config'])
255/38: dict(p['tsir_config'])
255/39: dict(p['tsir_config'])
255/40: dict(p['tsir_config']).items()
256/1: import pandas as pd; import numpy as np; import matplotlib.pyplot as plt
256/2: turbo = pd.read_csv("vacc_sim_5by5_2022-11-30T19:56:28.452165_TurBO_seed_center_final_solution.csv")
256/3: bo = pd.read_csv("vacc_sim_5by5_2022-11-30T22:36:49.756057_vanilla_BO_seed_center_final_solution.csv")
256/4: turbo
256/5: turbo = pd.read_csv("vacc_sim_5by5_2022-11-30T19:56:28.452165_TurBO_seed_center_final_solution.csv",header=None)
256/6: bo = pd.read_csv("vacc_sim_5by5_2022-11-30T22:36:49.756057_vanilla_BO_seed_center_final_solution.csv",header=None)
256/7: turbo
256/8: turbo[:,25]
256/9: turbo.iloc[:,25]
256/10: plt.plot(turbo.iloc[:,25])
256/11: %matplotlib qt
256/12: plt.plot(turbo.iloc[:,25])
256/13: plt.plot(bo.iloc[:,25])
256/14: turbo_f_evals = turbo.iloc[:,25]
256/15: bo_f_evals = bo.iloc[:,25]
256/16: len(bo_f_evals)
256/17: len(turbo_f_evals)
256/18: turbo_f_evals[0:1354]
256/19: len(turbo_f_evals[0:1354])
256/20: len(turbo_f_evals[0:1301])
256/21: plt.figure(); plt.plot(turbo_f_evals[0:1301]); plt.plot(bo_f_evals)
256/22: plt.figure(); plt.plot(turbo_f_evals[0:1301]); plt.plot(bo_f_evals)
256/23: plt.figure(); plt.plot(turbo_f_evals[0:1301]); plt.plot(bo_f_evals)
256/24: plt.figure(); plt.plot(turbo_f_evals[0:1301]); plt.plot(bo_f_evals)
256/25: plt.figure(); plt.plot(turbo_f_evals[0:1301], label="LA-MCTS w/ TurBO"); plt.plot(bo_f_evals,"BO")
256/26: plt.figure(); plt.plot(turbo_f_evals[0:1301], label="LA-MCTS w/ TurBO"); plt.plot(bo_f_evals,label="BO")
256/27: plt.figure(); plt.plot(turbo_f_evals[0:1301], label="LA-MCTS w/ TurBO"); plt.plot(bo_f_evals,label="BO"), plt.legend()
256/28: plt.figure(); plt.plot(turbo_f_evals[0:1301], label="LA-MCTS w/ TurBO"); plt.plot(bo_f_evals,label="BO"), plt.legend(); plt.title("5x5 disease simulation, seed_center, LA-MCTS vs. BO")
256/29: plt.figure(); plt.plot(turbo_f_evals[0:1301], label="LA-MCTS w/ TurBO"); plt.plot(bo_f_evals,label="BO"), plt.legend(); plt.title("5x5 disease simulation, seed_center, LA-MCTS vs. BO"); plt.xlabel("# of function evals"); plt.ylabel("Best f(x) so far")
256/30: plt.rc('font',size=15)
256/31: plt.figure(); plt.plot(turbo_f_evals[0:1301], label="LA-MCTS w/ TurBO"); plt.plot(bo_f_evals,label="BO"), plt.legend(); plt.title("5x5 disease simulation, seed_center, LA-MCTS vs. BO"); plt.xlabel("# of function evals"); plt.ylabel("Best f(x) so far")
256/32: turbo
256/33: turbo[1301,:]
256/34: turbo[.iloc1301,:]
256/35: turbo.iloc[1301,:]
256/36: turbo.iloc[1301,0:-1]
256/37: turbo.iloc[1301,0:-1].reshape(5,5)
256/38: np.array(turbo.iloc[1301,0:-1]).reshape(5,5)
256/39: np.array(bo.iloc[1301,0:-1]).reshape(5,5)
256/40: np.array(bo.iloc[-1,0:-1]).reshape(5,5)
256/41: plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5))
256/42: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5))
256/43: plt.figure(); plt.imshow(np.array(turbo.iloc[1301,0:-1]).reshape(5,5))
256/44: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1]).reshape(5,5))
256/45: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1],edgecolors='k').reshape(5,5))
256/46: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1]), edgecolors='k').reshape(5,5))
256/47: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1]), edgecolors='k').reshape(5,5)
256/48: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1]).reshape(5,5), edgecolors='k')
256/49: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1]).reshape(5,5).T, edgecolors='k')
256/50: plt.figure(); plt.imshow(np.array(turbo.iloc[1301,0:-1]).reshape(5,5))
256/51: plt.figure(); plt.imshow(np.array(turbo.iloc[1301,0:-1]).reshape(5,5)); plt.colorbar()
256/52: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar()
256/53: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar()
256/54: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar()
256/55: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar()
256/56: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
258/1:
import pandas as pd; import numpy as np; import matplotlib.pyplot as plt
turbo = pd.read_csv("vacc_sim_5by5_2022-11-30T19:56:28.452165_TurBO_seed_center_final_solution.csv")
bo = pd.read_csv("vacc_sim_5by5_2022-11-30T22:36:49.756057_vanilla_BO_seed_center_final_solution.csv")
258/2: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
258/3: %matplotlib qt
258/4: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
258/5: plt.figure(); plt.imshow(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
258/6: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5)); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
258/7: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
258/8: plt.rc('font',size=15)
258/9: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
259/1:
import pandas as pd; import numpy as np; import matplotlib.pyplot as plt
turbo = pd.read_csv("vacc_sim_5by5_2022-11-30T19:56:28.452165_TurBO_seed_center_final_solution.csv")
bo = pd.read_csv("vacc_sim_5by5_2022-11-30T22:36:49.756057_vanilla_BO_seed_center_final_solution.csv")
plt.rc(font='15')
259/2: plt.rc(font='15')
259/3: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
259/4: %matplotlib qt
259/5: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
259/6: plt.rc(font='15')
259/7: plt.rc('font',size=15)
259/8: plt.figure(); plt.pcolormesh(np.array(bo.iloc[-1,0:-1]).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("BO solution, 5x5 grid, seed_center")
259/9: plt.figure(); plt.pcolormesh(np.array(turbo.iloc[1301,0:-1]).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("LA-MCTS solution, 5x5 grid, seed_center")
260/1: import pandas as pd; import numpy as np; import matplotlib.pyplot as plt
260/2: pop = pd.read_csv("pop.csv")
260/3: pop
260/4: plt.pcolormesh(np.array(pop['pop']).reshape(5,5))
260/5: %matpl
260/6: %matplotlib qt
260/7: %matpl
260/8: plt.pcolormesh(np.array(pop['pop']).reshape(5,5))
260/9: plt.pcolormesh(np.array(pop['pop']).reshape(5,5), edgecolors="white"); plt.colorbar()
260/10: plt.figure(); plt.pcolormesh(np.array(pop['pop']).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("5x5 grid: Population")
260/11: plt.figure(); plt.pcolormesh(np.array(pop['vacc']).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("5x5 grid: % vaccination")
260/12: ls
260/13: pd.read_csv('seed_center.csv')
260/14: pd.read_csv('seed_center.csv',header=None)
260/15: plt.pcolorbar(np.array(pd.read_csv('seed_center.csv',header=None)).reshape(5,5))
260/16: plt.pcolormesh(np.array(pd.read_csv('seed_center.csv',header=None)).reshape(5,5), edgecolor="white")
260/17: plt.figure(); plt.pcolormesh(np.array(pd.read_csv('seed_center.csv',header=None)).reshape(5,5), edgecolor="white")
260/18: plt.figure(); plt.pcolormesh(np.array(pd.read_csv('seed_center.csv',header=None)).reshape(5,5), edgecolor="white"); plt.pcolorbar()
260/19: plt.figure(); plt.pcolormesh(np.array(pd.read_csv('seed_center.csv',header=None)).reshape(5,5), edgecolor="white"); plt.colorbar()
260/20: plt.figure(); plt.pcolormesh(np.array(pd.read_csv('seed_center.csv',header=None)).reshape(5,5), edgecolor="white"); plt.title("seed_center"); plt.colorbar()
260/21: plt.figure(); plt.pcolormesh(np.array(pop['vacc']).reshape(5,5), edgecolors="white"); plt.colorbar(); plt.title("5x5 grid: % vaccination")
261/1: import pandas as pd; import numpy as np; import matplotlib.pyplot as plt
261/2: %matplotlib qt
261/3: pd.read_csv('ZC_immunization_sifat.csv')
261/4: vacc_df = pd.read_csv('ZC_immunization_sifat.csv')
261/5: vacc_df['population']
261/6: plt.hist(vacc_df['population'])
261/7: plt.hist(vacc_df['population'])
261/8: vacc_df
261/9: vacc_df['population'] >= 0
261/10: vacc_df[vacc_df['population'] >= 0]
261/11: vacc_df[vacc_df['population'] > 0]
261/12: vacc_df[vacc_df['population'] > 0]['population']
261/13: plt.hist(vacc_df[vacc_df['population'] > 0]['population'])
   1: import pandas as pd; import numpy as np; import matplotlib.pyplot as plt
   2: np.arange(0,8*35,by=35)
   3: np.arange(0,8*35,length=8)
   4: help(np.arange)
   5: np.arange(0,8*35,step=35)
   6: coords = np.arange(0,8*35,step=35)
   7: coords = [(x,y) for x in coords for y in coords]
   8: coords
   9: pd.DataFrame(coords)
  10: coords = pd.DataFrame(coords)
  11: coords
  12: coords[0]
  13: coords.colnames = ['x','y']
  14: coords.columns = ['x','y']
  15: coords
  16: np.randon.normal
  17:
np.random.normal(
)
  18: np.random.normal()
  19: help(np.random.normal())
  20: help(np.random.normal)
  21: np.random.normal(loc=20000)
  22: np.random.normal(loc=20000,scale=5000)
  23: np.random.normal(loc=20000,scale=2500)
  24: np.random.normal(loc=20000,scale=2500)
  25: np.random.normal(loc=20000,scale=2500)
  26: np.random.normal(loc=20000,scale=2500)
  27: np.random.normal(loc=20000,scale=2500)
  28: np.random.normal(loc=20000,scale=2500)
  29: np.random.normal(loc=20000,scale=2500,length=64)
  30: help(np.random.normal)
  31: np.random.normal(loc=20000,scale=2500,size=64)
  32: np.round(np.random.normal(loc=20000,scale=2500,size=64))
  33: ls
  34: ls
  35: ls()
  36: coords
  37: coords['pop'] = np.round(np.random.normal(loc=20000,scale=2500,size=64))
  38: coords
  39: coords.iloc[0]
  40: ls
  41: coords.to_csv('pop.csv')
  42: np.random.normal(loc=400000,scale=1200)
  43: round(np.random.normal(loc=400000,scale=1200))
  44: round(np.random.normal(loc=400000,scale=1200))
  45: round(np.random.normal(loc=430000,scale=1200))
  46: round(np.random.normal(loc=430000,scale=1200))
  47: round(np.random.normal(loc=430000,scale=2500))
  48: round(np.random.normal(loc=430000,scale=2500))
  49: round(np.random.normal(loc=430000,scale=2500))
  50: round(np.random.normal(loc=430000,scale=2500))
  51: round(np.random.normal(loc=430000,scale=5000))
  52: 35*6
  53: 35*7
  54: round(np.random.normal(loc=430000,scale=5000))
  55: round(np.random.normal(loc=80000,scale=5000))
  56: round(np.random.normal(loc=80000,scale=5000))
  57: round(np.random.normal(loc=80000,scale=5000))
  58: round(np.random.normal(loc=80000,scale=5000),size=4)
  59: round(np.random.normal(loc=80000,scale=5000,size=4))
  60: round(np.random.normal(loc=80000,scale=5000,size=4))
  61: round(np.random.normal(size=4,loc=80000,scale=5000))
  62: np.round(np.random.normal(size=4,loc=80000,scale=5000))
  63: ls
  64: pop = pd.read_csv("pop.csv")
  65: pop
  66: pop['pop']
  67: np.array(pop['pop'])
  68: np.array(pop['pop'])[0:-1]
  69: pop_array = np.array(pop['pop'])[0:-1]
  70: plt.pcolormesh(pop_array.reshape(8,8), edgecolor="white")
  71: %matplotlib qt
  72: plt.pcolormesh(pop_array.reshape(8,8), edgecolor="white")
  73: plt.pcolormesh(pop_array.reshape(8,8), edgecolor="white"); plt.pcolorbar()
  74: plt.pcolormesh(pop_array.reshape(8,8), edgecolor="white"); plt.colorbar()
  75: np.random.normal(loc=10000,scale=5000)
  76: np.round(np.random.normal(loc=10000,scale=5000,size=7))
  77: np.round(np.random.normal(loc=10000,scale=2500,size=7))
  78: np.round(np.random.normal(loc=10000,scale=2000,size=7))
  79: np.round(np.random.normal(loc=10000,scale=2000,size=7))
  80: np.round(np.random.normal(loc=10000,scale=2000,size=7))
  81: np.round(np.random.normal(loc=10000,scale=2000,size=7))
  82: np.round(np.random.normal(loc=10000,scale=2000,size=7))
  83: pop_array = np.array(pop['pop'])[0:-1]
  84: pop = pd.read_csv("pop.csv")
  85: pop_array = np.array(pop['pop'])[0:-1]
  86: plt.pcolormesh(pop_array.reshape(8,8), edgecolor="white"); plt.colorbar()
  87: pop = pd.read_csv("pop.csv")
  88: poop
  89: pop
  90: pop_array = np.array(pop['pop'])[0:-1]
  91: plt.pcolormesh(pop_array.reshape(8,8), edgecolor="white"); plt.colorbar()
  92: np.random.normal(loc=0.95)
  93: np.random.normal(loc=0.95,scale=0.01)
  94: np.random.normal(loc=0.95,scale=0.01)
  95: np.random.normal(loc=0.95,scale=0.01)
  96: np.random.normal(loc=0.95,scale=0.01,size=64)
  97: np.random.normal(loc=0.94,scale=0.01,size=64)
  98: np.round(np.random.normal(loc=0.94,scale=0.01,size=64))
  99: help(np.round)
 100: np.round(np.random.normal(loc=0.94,scale=0.01,size=64),decimals=3)
 101: np.round(np.random.normal(loc=0.94,scale=0.001,size=64),decimals=3)
 102: np.round(np.random.normal(loc=0.94,scale=0.005,size=64),decimals=3)
 103: pop['vacc']=Out[102]
 104: pop.iloc[:-1]
 105: pop = pop.iloc[:-1]
 106: pop[1:,]
 107: pop.iloc[1:,]
 108: pop.iloc[1:,:]
 109: pop.iloc[1,1]
 110: pop.iloc[1,:]
 111: pop.iloc[:,:]
 112: pop.iloc[:,1:]
 113: pop = pop.iloc[:,1:]
 114: np.round(np.random.normal(loc=0.94,scale=0.005,size=64),decimals=3)
 115: pop['vacc'] = np.round(np.random.normal(loc=0.94,scale=0.005,size=64),decimals=3)
 116: pop
 117: pop.to_csv('pop.csv')
 118: pop['vacc']
 119: np.array(pop['vacc'])
 120: np.array(pop['vacc']).reshape(8,8)
 121: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 122: plt.imshow(np.array(pop['vacc']).reshape(8,8)); plt.colorbar()
 123: pop['vacc'] = np.round(np.random.normal(loc=0.95,scale=0.005,size=64),decimals=3)
 124: plt.imshow(np.array(pop['vacc']).reshape(8,8)); plt.colorbar()
 125: plt.imshow(np.array(pop['vacc']).reshape(8,8)); plt.colorbar()
 126: pop.to_csv('pop.csv')
 127: np.round(np.random.normal(loc=10000,scale=2000,size=7))
 128: pop['x','y']
 129: pop[['x','y']]
 130: list(pop[['x','y']])
 131: pop[['x','y']]
 132: [np.linalg.norm(x-y,ord=2) for x,y in zip(pop['x'],pop['y'])]
 133: pop[['x','y']].iloc(1)
 134: pop[['x','y']].iloc[1]
 135: pop[['x','y']].iloc[2]
 136: pop[['x','y']].iloc[3]
 137: np.linalg.norm(pop[['x','y']].iloc[3] - pop[['x','y']].iloc[4])
 138: np.linalg.norm(pop[['x','y']].iloc[3] - pop[['x','y']].iloc[4],ord=2)
 139: [np.linalg.norm(pop[['x','y']].iloc[i] - pop[['x','y']].iloc[j],ord=2) for i in range(0,8) for j in range(0,8)]
 140: np.array([np.linalg.norm(pop[['x','y']].iloc[i] - pop[['x','y']].iloc[j],ord=2) for i in range(0,8) for j in range(0,8)]).reshape(8,8)
 141: np.array([np.linalg.norm(pop[['x','y']].iloc[i] - pop[['x','y']].iloc[j],ord=2) for i in range(0,64) for j in range(0,64)]).reshape(64,64)
 142: np.array([np.linalg.norm(pop[['x','y']].iloc[i] - pop[['x','y']].iloc[j],ord=2) for i in range(0,64) for j in range(0,64)]).reshape(64,64)
 143: pd.DataFrame(Out[141])
 144: pd.DataFrame(Out[141]).to_csv('dist_mat.csv')
 145: pop = pd.read_csv("pop.csv")
 146: pop['vacc']
 147: np.array(pop['vacc'])
 148: np.array(pop['vacc']).reshape(8,8)
 149: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 150: np.array(pop['vacc']).reshape(8,8)
 151: pop = pd.read_csv("pop.csv")
 152: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 153: pop = pd.read_csv("pop.csv")
 154: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 155: pop = pd.read_csv("pop.csv")
 156: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 157: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 158: pop = pd.read_csv("pop.csv")
 159: plt.imshow(np.array(pop['vacc']).reshape(8,8))
 160: 64**2
 161: 700**2
 162: [(i,j,np.linalg.norm( pop[['x','y']].iloc[i] - pop[['x','y']].iloc[j] )) for i in range(0,64) for j in range(0,64)]
 163: dist_list = [(i,j,np.linalg.norm( pop[['x','y']].iloc[i] - pop[['x','y']].iloc[j] )) for i in range(0,64) for j in range(0,64)]
 164: pd.DataFrame(dist_list)
 165: pd.DataFrame(dist_list).to_csv('dist.csv')
 166: np.random.randint
 167: np.random.randint()
 168: np.random.randint(0,10)
 169: np.random.randint(0,64)
 170: np.random.randint(0,64,size=10)
 171: np.zeros(64)
 172: y = np.zeros(64)
 173: np.put(y,np.random.randint(0,64,size=10),1)
 174: y
 175: np.put(y,np.random.choice(list(range(0,64)),size=10),1)
 176: y
 177: y= np.zeros(64)
 178: np.put(y,np.random.choice(list(range(0,64)),size=10),1)
 179: y
 180: sum(y)
 181: np.put(y,np.random.choice(list(range(0,64),replace=False),size=10),1)
 182: np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),1)
 183: y
 184: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),1)
 185: y
 186: sum(y)
 187: plt.imshow(y.reshape(8,8))
 188: plt.figure(); plt.imshow(y.reshape(8,8))
 189: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),1)
 190: plt.figure(); plt.imshow(y.reshape(8,8))
 191: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),1)
 192: plt.figure(); plt.imshow(y.reshape(8,8))
 193: plt.figure(); plt.imshow(y.reshape(8,8))
 194: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),1)
 195: plt.figure(); plt.imshow(y.reshape(8,8))
 196: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),5)
 197: plt.figure(); plt.imshow(y.reshape(8,8))
 198: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),5)
 199: plt.figure(); plt.imshow(y.reshape(8,8))
 200: pd.DataFrame(y)
 201: pd.DataFrame(y).to_csv("seed_random_1.csv")
 202: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),5)
 203: plt.figure(); plt.imshow(y.reshape(8,8))
 204: pd.DataFrame(y).to_csv("seed_random_2.csv")
 205: y = np.zeros(64); np.put(y,np.random.choice(list(range(0,64)),size=10,replace=False),5)
 206: y = np.zeros(64); np.put(y,0,5)
 207: np.put(y,54,5)
 208: plt.imshow(y.reshape(5,5))
 209: plt.imshow(y.reshape(8,8))
 210: pd.DataFrame(y).to_csv("seed_centers.csv")
 211: y = np.zeros(64)
 212: %history -g -f create_env_ipython_history.txt
 213: y = np.zeros(64)
 214: np.put(y,56,5)
 215: np.put(y,7,56)
 216: plt.imshow(y.reshape(8,8))
 217: y
 218: y[7]=5
 219: plt.imshow(y.reshape(8,8))
 220: pd.DataFrame(y).to_csv("seed_clusters.csv")
 221: %history -g -f create_env_ipython_history.txt
